{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d05f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 20:44:06.271919: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-02 20:44:06.296634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746218646.322150 1162385 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746218646.330752 1162385 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746218646.352937 1162385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746218646.352961 1162385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746218646.352964 1162385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746218646.352966 1162385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-02 20:44:06.358653: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "DIM_IN = 2048\n",
    "DIM_HIDDEN = 4096\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, d_in=DIM_IN, d_hidden=DIM_HIDDEN):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_in, d_hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.decoder = nn.Linear(d_hidden, d_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.encoder(x))\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "dtype = torch.float16\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(1)  # Set current device to 1\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    dtype = torch.bfloat16\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "144b60b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70299c95eea64c8c8e49c14024401cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"google/paligemma-3b-mix-224\"\n",
    "# TRAINING_DATASET = \"imagenet-1k\"\n",
    "# TRAINING_DATASET_SIZE = 5000\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype\n",
    ").eval()\n",
    "model = model.to(device) # Move model to GPU\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Set up activation capture (vision tower)\n",
    "vision_acts = {}\n",
    "def vision_hook(module, input, output):\n",
    "    vision_acts[\"activation\"] = output\n",
    "\n",
    "hook_handle = model.vision_tower.vision_model.encoder.layers[20].register_forward_hook(vision_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55dd8c",
   "metadata": {},
   "source": [
    "Load in DROID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004da5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746218740.358063 1162385 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 891 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n",
      "I0000 00:00:1746218740.359922 1162385 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 26284 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:05.0, compute capability: 8.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "import html\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Settings\n",
    "SEQLEN = 128\n",
    "NUM_FRAMES = 8  # üîÅ Change this to however many frames you want\n",
    "\n",
    "# --- Load dataset ---\n",
    "train_ds = tfds.load(\"droid_100\", split=\"train[:90%]\", data_dir=\"gs://gresearch/robotics\")\n",
    "val_ds = tfds.load(\"droid_100\", split=\"train[90%:]\", data_dir=\"gs://gresearch/robotics\")\n",
    "\n",
    "# --- Image preprocessing ---\n",
    "def preprocess_image(image):\n",
    "    return image.resize((224, 224))  # for model input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e315659c",
   "metadata": {},
   "source": [
    "Train SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Initialize Weights & Biases ===\n",
    "wandb.init(project=\"sparse-autoencoder\", name=\"SAE-run\", config={\n",
    "    \"hidden_multiplier\": 8,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"sparsity_weight\": 1e-2,\n",
    "    \"n_epochs\": 50,\n",
    "    \"batch_size\": 512,\n",
    "})\n",
    "\n",
    "# === Hyperparameters ===\n",
    "hidden_multiplier = wandb.config.hidden_multiplier\n",
    "learning_rate = wandb.config.learning_rate\n",
    "sparsity_weight = wandb.config.sparsity_weight\n",
    "n_epochs = wandb.config.n_epochs\n",
    "batch_size = wandb.config.batch_size\n",
    "\n",
    "# === Prepare SAE ===\n",
    "d_in = all_activations.shape[-1]\n",
    "d_hidden = hidden_multiplier * d_in\n",
    "\n",
    "sae = SparseAutoencoder(d_in=d_in, d_hidden=d_hidden).to(device) \n",
    "sae = nn.DataParallel(sae)  # Now wrap for multi-GPU\n",
    "optimizer = torch.optim.AdamW(sae.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# === Dataset & Loader ===\n",
    "train_dataset = torch.utils.data.TensorDataset(all_activations)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
