{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned PaliGemma SAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define SAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:7\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'big_vision'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mml_collections\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentencepiece\u001b[39;00m  \u001b[38;5;66;03m# Used in tokenization (not used yet)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbig_vision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproj\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpaligemma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m paligemma\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# === Constants ===\u001b[39;00m\n\u001b[32m     85\u001b[39m LLM_VARIANT = \u001b[33m\"\u001b[39m\u001b[33mgemma2_2b\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'big_vision'"
     ]
    }
   ],
   "source": [
    "# === Standard Library ===\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "from threading import Thread\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import wandb\n",
    "import multiprocessing\n",
    "\n",
    "# === PyTorch ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torch.cuda.amp import autocast\n",
    "import torch\n",
    "\n",
    "# === Transformers & Datasets ===\n",
    "from transformers import (\n",
    "    AutoModelForVision2Seq,\n",
    "    AutoProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# === TorchVision ===\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# === TensorFlow (if needed) ===\n",
    "import tensorflow as tf\n",
    "\n",
    "DIM_IN = 2048\n",
    "DIM_HIDDEN = 4096\n",
    "\n",
    "# === Custom Dataset ===\n",
    "class PromptImageDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, d_in=DIM_IN, d_hidden=DIM_HIDDEN):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_in, d_hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.decoder = nn.Linear(d_hidden, d_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.encoder(x))\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "dtype = torch.float16\n",
    "device_num = 7\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device_num)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    dtype = torch.bfloat16\n",
    "\n",
    "device = f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "import os\n",
    "import flax\n",
    "import jax\n",
    "import ml_collections\n",
    "import sentencepiece  # Used in tokenization (not used yet)\n",
    "from big_vision.models.proj.paligemma import paligemma\n",
    "\n",
    "# === Constants ===\n",
    "LLM_VARIANT = \"gemma2_2b\"\n",
    "VOCAB_SIZE = 257_152\n",
    "IMG_ENCODER_VARIANT = \"So400m/14\"\n",
    "CHECKPOINT_DIR = \"/home/henrytsai/dhruv\"\n",
    "CHECKPOINT_STEP = 5  # Change this to load a different checkpoint\n",
    "MODEL_PATH = \"/home/henrytsai/.cache/kagglehub/models/google/paligemma-2/jax/paligemma2-3b-pt-896/1/./paligemma2-3b-pt-896.b16.npz\"  # <-- You need to set this to your model's path\n",
    "\n",
    "# === Model configuration ===\n",
    "# Define architecture for language and image components\n",
    "model_config = ml_collections.FrozenConfigDict({\n",
    "    \"llm\": {\n",
    "        \"vocab_size\": VOCAB_SIZE,\n",
    "        \"variant\": LLM_VARIANT,\n",
    "        \"final_logits_softcap\": 0.0\n",
    "    },\n",
    "    \"img\": {\n",
    "        \"variant\": IMG_ENCODER_VARIANT,\n",
    "        \"pool_type\": \"none\",\n",
    "        \"scan\": True,\n",
    "        \"dtype_mm\": \"float16\"  # Use half-precision for multimodal encoder\n",
    "    }\n",
    "})\n",
    "\n",
    "# === Initialize empty model parameters ===\n",
    "# This sets up the parameter tree structure (but doesn't load trained weights yet)\n",
    "init_params = paligemma.load(None, MODEL_PATH, model_config)\n",
    "\n",
    "# === Checkpoint loading function ===\n",
    "def load_checkpoint(template_params, step, save_dir=CHECKPOINT_DIR):\n",
    "    \"\"\"\n",
    "    Load Flax model parameters from a msgpack checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "        template_params: parameter tree to match structure of saved checkpoint.\n",
    "        step: training step number of the checkpoint (e.g., 1 for checkpoint_0001).\n",
    "        save_dir: path to the directory containing checkpoint files.\n",
    "\n",
    "    Returns:\n",
    "        A PyTree of model parameters with trained weights loaded.\n",
    "    \"\"\"\n",
    "    filename = f\"checkpoint_{step:04d}.msgpack\"\n",
    "    path = os.path.join(save_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {path}\")\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        raw_bytes = f.read()\n",
    "    \n",
    "    return flax.serialization.from_bytes(template_params, raw_bytes)\n",
    "\n",
    "# === Load trained model parameters ===\n",
    "print(\"Starting model parameter loading\")\n",
    "params = load_checkpoint(init_params, step=CHECKPOINT_STEP)\n",
    "print(\"Finished model parameter loading\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtfds\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_droid_subset\u001b[39m(return_dict):\n\u001b[32m     36\u001b[39m     ds = tfds.load(\u001b[33m\"\u001b[39m\u001b[33mdroid_100\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, data_dir=\u001b[33m\"\u001b[39m\u001b[33mgs://gresearch/robotics\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "# --------- helpers you already wrote ----------\n",
    "NUM_FRAMES = 6               # grid uses evenly spaced frames\n",
    "FRAMES_PER_ROW = 3          \n",
    "\n",
    "def preprocess_image(im: Image.Image) -> Image.Image:\n",
    "    return im.resize((896, 896), Image.BILINEAR)   # <‑‑ used ONLY for display\n",
    "\n",
    "def subsample_frames(images, num_samples):\n",
    "    if len(images) <= num_samples:\n",
    "        return images\n",
    "    idx = np.linspace(0, len(images)-1, num=num_samples, dtype=int)\n",
    "    return [images[i] for i in idx]\n",
    "\n",
    "def stack_images_horizontally(imgs):\n",
    "    w, h = zip(*(im.size for im in imgs))\n",
    "    canvas = Image.new(\"RGB\", (sum(w), max(h)))\n",
    "    x = 0\n",
    "    for im in imgs:\n",
    "        canvas.paste(im, (x, 0))\n",
    "        x += im.width\n",
    "    return canvas\n",
    "\n",
    "def stack_images_grid(imgs, frames_per_row=FRAMES_PER_ROW):\n",
    "    rows = [imgs[i:i+frames_per_row] for i in range(0, len(imgs), frames_per_row)]\n",
    "    rows = [stack_images_horizontally(r) for r in rows]\n",
    "    as_np = np.vstack([np.asarray(r) for r in rows])\n",
    "    return Image.fromarray(as_np)\n",
    "# ----------------------------------------------\n",
    "\n",
    "### Loading DROID dataset\n",
    "import multiprocessing as mp\n",
    "from PIL import Image\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def load_droid_subset(return_dict):\n",
    "    ds = tfds.load(\"droid_100\", split=\"train\", data_dir=\"gs://gresearch/robotics\")\n",
    "\n",
    "    frames, prompts = [], []\n",
    "    for episode in ds.take(100):                        # ←‑‑ keep your 10‑episode cap\n",
    "        steps = list(episode[\"steps\"])\n",
    "\n",
    "        # ------- build the 8‑frame grid -------\n",
    "        all_imgs = [Image.fromarray(step[\"observation\"][\"wrist_image_left\"].numpy())\n",
    "                    for step in steps]\n",
    "\n",
    "        # pick evenly spaced frames\n",
    "        sampled = subsample_frames(all_imgs, NUM_FRAMES)\n",
    "\n",
    "        # build H×W grid (typically 4×2) and resize to 896x896\n",
    "        grid_pil  = stack_images_grid(sampled)          # produced by helper above\n",
    "        grid_pil  = grid_pil.resize((896, 896), Image.BILINEAR)\n",
    "\n",
    "        # --------------------------------------\n",
    "\n",
    "        frames.append(grid_pil)     # **store the PIL grid**\n",
    "        prompts.append(steps[0][\"language_instruction\"].numpy().decode(\"utf-8\"))\n",
    "\n",
    "    return_dict[\"frames\"]  = frames\n",
    "    return_dict[\"prompts\"] = prompts\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "return_dict = manager.dict()\n",
    "p = multiprocessing.Process(target=load_droid_subset, args=(return_dict,))\n",
    "p.start()\n",
    "p.join()\n",
    "\n",
    "frames, prompts = return_dict[\"frames\"], return_dict[\"prompts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
