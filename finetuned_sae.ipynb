{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned PaliGemma SAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define SAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:7\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'big_vision'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mml_collections\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentencepiece\u001b[39;00m  \u001b[38;5;66;03m# Used in tokenization (not used yet)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbig_vision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproj\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpaligemma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m paligemma\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# === Constants ===\u001b[39;00m\n\u001b[32m     85\u001b[39m LLM_VARIANT = \u001b[33m\"\u001b[39m\u001b[33mgemma2_2b\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'big_vision'"
     ]
    }
   ],
   "source": [
    "# === Standard Library ===\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "from threading import Thread\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import wandb\n",
    "import multiprocessing\n",
    "\n",
    "# === PyTorch ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torch.cuda.amp import autocast\n",
    "import torch\n",
    "\n",
    "# === Transformers & Datasets ===\n",
    "from transformers import (\n",
    "    AutoModelForVision2Seq,\n",
    "    AutoProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# === TorchVision ===\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# === TensorFlow (if needed) ===\n",
    "import tensorflow as tf\n",
    "\n",
    "DIM_IN = 2048\n",
    "DIM_HIDDEN = 4096\n",
    "\n",
    "# === Custom Dataset ===\n",
    "class PromptImageDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, d_in=DIM_IN, d_hidden=DIM_HIDDEN):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_in, d_hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.decoder = nn.Linear(d_hidden, d_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.encoder(x))\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "dtype = torch.float16\n",
    "device_num = 7\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device_num)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    dtype = torch.bfloat16\n",
    "\n",
    "device = f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "import os\n",
    "import flax\n",
    "import jax\n",
    "import ml_collections\n",
    "import sentencepiece  # Used in tokenization (not used yet)\n",
    "from big_vision.models.proj.paligemma import paligemma\n",
    "\n",
    "# === Constants ===\n",
    "LLM_VARIANT = \"gemma2_2b\"\n",
    "VOCAB_SIZE = 257_152\n",
    "IMG_ENCODER_VARIANT = \"So400m/14\"\n",
    "CHECKPOINT_DIR = \"/home/henrytsai/dhruv\"\n",
    "CHECKPOINT_STEP = 5  # Change this to load a different checkpoint\n",
    "MODEL_PATH = \"/home/henrytsai/.cache/kagglehub/models/google/paligemma-2/jax/paligemma2-3b-pt-896/1/./paligemma2-3b-pt-896.b16.npz\"  # <-- You need to set this to your model's path\n",
    "\n",
    "# === Model configuration ===\n",
    "# Define architecture for language and image components\n",
    "model_config = ml_collections.FrozenConfigDict({\n",
    "    \"llm\": {\n",
    "        \"vocab_size\": VOCAB_SIZE,\n",
    "        \"variant\": LLM_VARIANT,\n",
    "        \"final_logits_softcap\": 0.0\n",
    "    },\n",
    "    \"img\": {\n",
    "        \"variant\": IMG_ENCODER_VARIANT,\n",
    "        \"pool_type\": \"none\",\n",
    "        \"scan\": True,\n",
    "        \"dtype_mm\": \"float16\"  # Use half-precision for multimodal encoder\n",
    "    }\n",
    "})\n",
    "\n",
    "# === Initialize empty model parameters ===\n",
    "# This sets up the parameter tree structure (but doesn't load trained weights yet)\n",
    "init_params = paligemma.load(None, MODEL_PATH, model_config)\n",
    "\n",
    "# === Checkpoint loading function ===\n",
    "def load_checkpoint(template_params, step, save_dir=CHECKPOINT_DIR):\n",
    "    \"\"\"\n",
    "    Load Flax model parameters from a msgpack checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "        template_params: parameter tree to match structure of saved checkpoint.\n",
    "        step: training step number of the checkpoint (e.g., 1 for checkpoint_0001).\n",
    "        save_dir: path to the directory containing checkpoint files.\n",
    "\n",
    "    Returns:\n",
    "        A PyTree of model parameters with trained weights loaded.\n",
    "    \"\"\"\n",
    "    filename = f\"checkpoint_{step:04d}.msgpack\"\n",
    "    path = os.path.join(save_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {path}\")\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        raw_bytes = f.read()\n",
    "    \n",
    "    return flax.serialization.from_bytes(template_params, raw_bytes)\n",
    "\n",
    "# === Load trained model parameters ===\n",
    "params = load_checkpoint(init_params, step=CHECKPOINT_STEP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
