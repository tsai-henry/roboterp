{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned PaliGemma SAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define SAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import flax.serialization\n",
    "import msgpack\n",
    "# TPUs with\n",
    "# if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "#   raise \"It seems you are using Colab with remote TPUs which is not supported.\"\n",
    "\n",
    "# Fetch big_vision repository if python doesn't know about it and install\n",
    "# dependencies needed for this notebook.\n",
    "if not os.path.exists(\"big_vision_repo\"):\n",
    "  !git clone --quiet --branch=main --depth=1 \\\n",
    "     https://github.com/google-research/big_vision big_vision_repo\n",
    "\n",
    "# Append big_vision code to python import path\n",
    "if \"big_vision_repo\" not in sys.path:\n",
    "  sys.path.append(\"big_vision_repo\")\n",
    "\n",
    "# Install missing dependencies. Assume jax~=0.4.25 with GPU available.\n",
    "!pip3 install -q \"overrides\" \"ml_collections\" \"einops~=0.7\" \"sentencepiece\"\n",
    "\n",
    "# === Standard Library ===\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "from threading import Thread\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import wandb\n",
    "import multiprocessing\n",
    "\n",
    "# === PyTorch ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torch.cuda.amp import autocast\n",
    "import torch\n",
    "\n",
    "# === Transformers & Datasets ===\n",
    "from transformers import (\n",
    "    AutoModelForVision2Seq,\n",
    "    AutoProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# === TorchVision ===\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# === TensorFlow (if needed) ===\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import flax\n",
    "import jax\n",
    "import ml_collections\n",
    "import sentencepiece  # Used in tokenization (not used yet)\n",
    "from big_vision.models.proj.paligemma import paligemma\n",
    "import multiprocessing as mp\n",
    "from PIL import Image\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:4\n"
     ]
    }
   ],
   "source": [
    "device_num = 4 # CUDA GPU number\n",
    "\n",
    "DIM_IN = 2048\n",
    "DIM_HIDDEN = 4096\n",
    "LLM_VARIANT = \"gemma2_2b\"\n",
    "VOCAB_SIZE = 257_152\n",
    "IMG_ENCODER_VARIANT = \"So400m/14\"\n",
    "CHECKPOINT_DIR = \"/home/henrytsai/dhruv\"\n",
    "CHECKPOINT_STEP = 5  # Change this to load a different checkpoint\n",
    "MODEL_PATH = \"/home/henrytsai/.cache/kagglehub/models/google/paligemma-2/jax/paligemma2-3b-pt-896/1/./paligemma2-3b-pt-896.b16.npz\"  # <-- You need to set this to your model's path\n",
    "\n",
    "\n",
    "# === Custom Dataset ===\n",
    "class PromptImageDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, d_in=DIM_IN, d_hidden=DIM_HIDDEN):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_in, d_hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.decoder = nn.Linear(d_hidden, d_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.encoder(x))\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "# === Checkpoint loading function ===\n",
    "def load_checkpoint(template_params, step, save_dir=CHECKPOINT_DIR):\n",
    "    \"\"\"\n",
    "    Load Flax model parameters from a msgpack checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "        template_params: parameter tree to match structure of saved checkpoint.\n",
    "        step: training step number of the checkpoint (e.g., 1 for checkpoint_0001).\n",
    "        save_dir: path to the directory containing checkpoint files.\n",
    "\n",
    "    Returns:\n",
    "        A PyTree of model parameters with trained weights loaded.\n",
    "    \"\"\"\n",
    "    filename = f\"checkpoint_{step:04d}.msgpack\"\n",
    "    path = os.path.join(save_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {path}\")\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        raw_bytes = f.read()\n",
    "    \n",
    "    return flax.serialization.from_bytes(template_params, raw_bytes)\n",
    "\n",
    "# === Model configuration ===\n",
    "# Define architecture for language and image components\n",
    "model_config = ml_collections.FrozenConfigDict({\n",
    "    \"llm\": {\n",
    "        \"vocab_size\": VOCAB_SIZE,\n",
    "        \"variant\": LLM_VARIANT,\n",
    "        \"final_logits_softcap\": 0.0\n",
    "    },\n",
    "    \"img\": {\n",
    "        \"variant\": IMG_ENCODER_VARIANT,\n",
    "        \"pool_type\": \"none\",\n",
    "        \"scan\": True,\n",
    "        \"dtype_mm\": \"float16\"  # Use half-precision for multimodal encoder\n",
    "    }\n",
    "})\n",
    "\n",
    "dtype = torch.float16\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device_num)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    dtype = torch.bfloat16\n",
    "\n",
    "device = f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model (40sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter config in `PaliGemmaForConditionalGeneration(config)` should be an instance of class `PretrainedConfig`. To create a model from a pretrained model use `model = PaliGemmaForConditionalGeneration.from_pretrained(PRETRAINED_MODEL_NAME)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load model structure (same config as used during training)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mPaliGemmaForConditionalGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load weights from saved checkpoint (.pt file)\u001b[39;00m\n\u001b[32m      5\u001b[39m state_dict = torch.load(\u001b[33m\"\u001b[39m\u001b[33m/home/henrytsai/dhruv/roboterp/finetuned_paligemma.pt\u001b[39m\u001b[33m\"\u001b[39m, map_location=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/henry/lib/python3.13/site-packages/transformers/models/paligemma/modeling_paligemma.py:297\u001b[39m, in \u001b[36mPaliGemmaForConditionalGeneration.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: PaliGemmaConfig):\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28mself\u001b[39m.vision_tower = AutoModel.from_config(config=config.vision_config)\n\u001b[32m    299\u001b[39m     \u001b[38;5;28mself\u001b[39m.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/henry/lib/python3.13/site-packages/transformers/modeling_utils.py:1860\u001b[39m, in \u001b[36mPreTrainedModel.__init__\u001b[39m\u001b[34m(self, config, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m   1859\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[32m-> \u001b[39m\u001b[32m1860\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1861\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParameter config in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(config)` should be an instance of class \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1862\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`PretrainedConfig`. To create a model from a pretrained model use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1863\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`model = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1864\u001b[39m     )\n\u001b[32m   1865\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_attn_implementation_autoset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1866\u001b[39m     \u001b[38;5;66;03m# config usually has a `torch_dtype` but we need the next line for the `no_super_init` tests\u001b[39;00m\n\u001b[32m   1867\u001b[39m     dtype = config.torch_dtype \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m torch.get_default_dtype()\n",
      "\u001b[31mValueError\u001b[39m: Parameter config in `PaliGemmaForConditionalGeneration(config)` should be an instance of class `PretrainedConfig`. To create a model from a pretrained model use `model = PaliGemmaForConditionalGeneration.from_pretrained(PRETRAINED_MODEL_NAME)`"
     ]
    }
   ],
   "source": [
    "# Load model structure (same config as used during training)\n",
    "model = PaliGemmaForConditionalGeneration(model_config).to(device)\n",
    "\n",
    "# Load weights from saved checkpoint (.pt file)\n",
    "state_dict = torch.load(\"/home/henrytsai/dhruv/roboterp/finetuned_paligemma.pt\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset (30sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 18:09:20.831322: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\n"
     ]
    }
   ],
   "source": [
    "# --------- helpers you already wrote ----------\n",
    "NUM_FRAMES = 6               # grid uses evenly spaced frames\n",
    "FRAMES_PER_ROW = 3          \n",
    "\n",
    "def preprocess_image(im: Image.Image) -> Image.Image:\n",
    "    return im.resize((896, 896), Image.BILINEAR)   # <‑‑ used ONLY for display\n",
    "\n",
    "def subsample_frames(images, num_samples):\n",
    "    if len(images) <= num_samples:\n",
    "        return images\n",
    "    idx = np.linspace(0, len(images)-1, num=num_samples, dtype=int)\n",
    "    return [images[i] for i in idx]\n",
    "\n",
    "def stack_images_horizontally(imgs):\n",
    "    w, h = zip(*(im.size for im in imgs))\n",
    "    canvas = Image.new(\"RGB\", (sum(w), max(h)))\n",
    "    x = 0\n",
    "    for im in imgs:\n",
    "        canvas.paste(im, (x, 0))\n",
    "        x += im.width\n",
    "    return canvas\n",
    "\n",
    "def stack_images_grid(imgs, frames_per_row=FRAMES_PER_ROW):\n",
    "    rows = [imgs[i:i+frames_per_row] for i in range(0, len(imgs), frames_per_row)]\n",
    "    rows = [stack_images_horizontally(r) for r in rows]\n",
    "    as_np = np.vstack([np.asarray(r) for r in rows])\n",
    "    return Image.fromarray(as_np)\n",
    "\n",
    "def load_droid_subset(return_dict):\n",
    "    ds = tfds.load(\"droid_100\", split=\"train\", data_dir=\"gs://gresearch/robotics\")\n",
    "\n",
    "    frames, prompts = [], []\n",
    "    for episode in ds.take(100):                        # ←‑‑ keep your 10‑episode cap\n",
    "        steps = list(episode[\"steps\"])\n",
    "\n",
    "        # ------- build the 8‑frame grid -------\n",
    "        all_imgs = [Image.fromarray(step[\"observation\"][\"wrist_image_left\"].numpy())\n",
    "                    for step in steps]\n",
    "\n",
    "        # pick evenly spaced frames\n",
    "        sampled = subsample_frames(all_imgs, NUM_FRAMES)\n",
    "\n",
    "        # build H×W grid (typically 4×2) and resize to 896x896\n",
    "        grid_pil  = stack_images_grid(sampled)          # produced by helper above\n",
    "        grid_pil  = grid_pil.resize((896, 896), Image.BILINEAR)\n",
    "\n",
    "        frames.append(grid_pil)     # **store the PIL grid**\n",
    "        prompts.append(steps[0][\"language_instruction\"].numpy().decode(\"utf-8\"))\n",
    "\n",
    "    return_dict[\"frames\"]  = frames\n",
    "    return_dict[\"prompts\"] = prompts\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "return_dict = manager.dict()\n",
    "p = multiprocessing.Process(target=load_droid_subset, args=(return_dict,))\n",
    "p.start()\n",
    "p.join()\n",
    "\n",
    "frames, prompts = return_dict[\"frames\"], return_dict[\"prompts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
