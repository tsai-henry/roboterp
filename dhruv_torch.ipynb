{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned PaliGemma SAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fine-tune PaLI-Gemma 2 3B on action-language data from the DROID dataset and train sparse autoencoders (SAEs) on its hidden activations to analyze feature changes. All training is performed using a single NVIDIA A100 40GB GPU running on Ubuntu 22.04 with CUDA 12.8.\n",
    "\n",
    "To reproduce this work, please download our fine-tuned checkpoint from https://github.com/tsai-henry/roboterp and install all dependencies using pip install -r requirements.txt. You will also need to install and login to Weights & Biases for storing training logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define helper functions and constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForVision2Seq,\n",
    "    AutoProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    "    PretrainedConfig\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import gc\n",
    "import wandb\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "CUDA_DEVICE_INDEX = 3 # Replace with 0 for a single GPU machine\n",
    "FINETUNE_CHECKPOINT_PATH = \"/home/henrytsai/dhruv/roboterp/finetuned_paligemma.pt\" # Replace this path\n",
    "\n",
    "MODEL_ID = \"google/paligemma2-3b-pt-896\"\n",
    "LAYER_IDX = -1\n",
    "NUM_FRAMES = 6               \n",
    "FRAMES_PER_ROW = 3\n",
    "\n",
    "DROID_DATASET_NAME = \"droid_100\"\n",
    "DROID_DATASET_SPLIT = \"train\"\n",
    "DROID_DATASET_GCS_DIR = \"gs://gresearch/robotics\"\n",
    "NUM_EPISODES = 100\n",
    "NUM_FRAMES = 6\n",
    "TARGET_IMAGE_SIZE = (896, 896)\n",
    "DIM_IN = 2048\n",
    "DIM_HIDDEN = 4096\n",
    "\n",
    "# === Dataset loading and preprocessing ===\n",
    "def load_droid_subset(num_episodes=100):\n",
    "    ds = tfds.load(DROID_DATASET_NAME, split=DROID_DATASET_SPLIT, data_dir=DROID_DATASET_GCS_DIR)\n",
    "\n",
    "    frames, prompts = [], []\n",
    "    for episode in ds.take(num_episodes):\n",
    "        steps = list(episode[\"steps\"])\n",
    "\n",
    "        # Extract and process wrist camera images\n",
    "        all_imgs = [Image.fromarray(step[\"observation\"][\"wrist_image_left\"].numpy())\n",
    "                    for step in steps]\n",
    "        sampled = subsample_frames(all_imgs, NUM_FRAMES)\n",
    "\n",
    "        # Build grid and resize\n",
    "        grid_pil = stack_images_grid(sampled).resize(TARGET_IMAGE_SIZE, Image.BILINEAR)\n",
    "\n",
    "        frames.append(grid_pil)\n",
    "        prompts.append(steps[0][\"language_instruction\"].numpy().decode(\"utf-8\"))\n",
    "\n",
    "    return frames, prompts\n",
    "\n",
    "vision_acts = {}\n",
    "\n",
    "def vision_hook(module, input, output):\n",
    "    vision_acts[\"activation\"] = output\n",
    "\n",
    "def preprocess_image(im: Image.Image) -> Image.Image:\n",
    "    return im.resize(TARGET_IMAGE_SIZE, Image.BILINEAR)   # <‑‑ used ONLY for display\n",
    "\n",
    "def subsample_frames(images, num_samples):\n",
    "    if len(images) <= num_samples:\n",
    "        return images\n",
    "    idx = np.linspace(0, len(images)-1, num=num_samples, dtype=int)\n",
    "    return [images[i] for i in idx]\n",
    "\n",
    "def stack_images_horizontally(imgs):\n",
    "    w, h = zip(*(im.size for im in imgs))\n",
    "    canvas = Image.new(\"RGB\", (sum(w), max(h)))\n",
    "    x = 0\n",
    "    for im in imgs:\n",
    "        canvas.paste(im, (x, 0))\n",
    "        x += im.width\n",
    "    return canvas\n",
    "\n",
    "def stack_images_grid(imgs, frames_per_row=FRAMES_PER_ROW):\n",
    "    rows = [imgs[i:i+frames_per_row] for i in range(0, len(imgs), frames_per_row)]\n",
    "    rows = [stack_images_horizontally(r) for r in rows]\n",
    "    as_np = np.vstack([np.asarray(r) for r in rows])\n",
    "    return Image.fromarray(as_np)\n",
    "\n",
    "# === Dataset wrapper ===\n",
    "class PromptImageDataset(Dataset):\n",
    "    def __init__(self, frames, prompts):\n",
    "        self.frames = frames\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.frames[idx], self.prompts[idx]\n",
    "\n",
    "# === Custom collate function ===\n",
    "def collate_fn(batch):\n",
    "    batch_frames, batch_prompts = zip(*batch)  # unzip into two lists\n",
    "    return list(batch_frames), list(batch_prompts)\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, d_in=DIM_IN, d_hidden=DIM_HIDDEN):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_in, d_hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.decoder = nn.Linear(d_hidden, d_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.encoder(x))\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "dtype = torch.float16\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(CUDA_DEVICE_INDEX)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    dtype = torch.bfloat16\n",
    "\n",
    "device = f\"cuda:{CUDA_DEVICE_INDEX}\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load finetuned PaliGemma model (20sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd28bab13754aa89b2f1780fe742d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PaliGemmaForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(4096, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
       "    (linear): Linear(in_features=1152, out_features=2304, bias=True)\n",
       "  )\n",
       "  (language_model): Gemma2ForCausalLM(\n",
       "    (model): Gemma2Model(\n",
       "      (embed_tokens): Embedding(257216, 2304, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-25): 26 x Gemma2DecoderLayer(\n",
       "          (self_attn): Gemma2Attention(\n",
       "            (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          )\n",
       "          (mlp): Gemma2MLP(\n",
       "            (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (rotary_emb): Gemma2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2304, out_features=257216, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(f\"cuda:{CUDA_DEVICE_INDEX}\")\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load(FINETUNE_CHECKPOINT_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DROID Dataset (40sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 20:14:37.647044: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "frames, prompts = load_droid_subset(100)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, use_fast=True)\n",
    "hook = model.vision_tower.vision_model.encoder.layers[LAYER_IDX].register_forward_hook(vision_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect activations (45sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b633ef07a24882914e2263b3b90f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations: torch.Size([100, 4096, 1152])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1957"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Build dataloader ===\n",
    "dataset = PromptImageDataset(frames, prompts)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "all_activations = []\n",
    "token_counts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_frames, batch_prompts in tqdm(dataloader):\n",
    "        inputs = processor(\n",
    "            text=[f\"<image> {p}\" for p in batch_prompts],\n",
    "            images=batch_frames,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(device, torch.float16)\n",
    "\n",
    "        _ = model(**inputs)\n",
    "\n",
    "        raw_output = vision_acts[\"activation\"]\n",
    "        batch_acts = raw_output[0] if isinstance(raw_output, tuple) else raw_output\n",
    "        batch_acts = batch_acts.detach().cpu()\n",
    "\n",
    "        all_activations.append(batch_acts)\n",
    "        token_counts.append(len(batch_prompts))\n",
    "        \n",
    "# === Combine all activations ===\n",
    "activations = torch.cat(all_activations, dim=0)\n",
    "print(\"Collected activations:\", activations.shape)\n",
    "\n",
    "torch.save(activations, \"activations.pt\")\n",
    "activations_fp16 = activations.half()\n",
    "torch.save(activations_fp16, \"activations_fp16.pt\")\n",
    "torch.save(token_counts, \"token_counts.pt\")\n",
    "\n",
    "del activations\n",
    "vision_acts.clear()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunday</strong> at: <a href='https://wandb.ai/htsai2025/finetuned-paligemma/runs/ao5nodex' target=\"_blank\">https://wandb.ai/htsai2025/finetuned-paligemma/runs/ao5nodex</a><br> View project at: <a href='https://wandb.ai/htsai2025/finetuned-paligemma' target=\"_blank\">https://wandb.ai/htsai2025/finetuned-paligemma</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250504_200527-ao5nodex/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/henrytsai/henry/roboterp/wandb/run-20250504_200759-c56lh1ha</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/htsai2025/finetuned-paligemma/runs/c56lh1ha' target=\"_blank\">sunday</a></strong> to <a href='https://wandb.ai/htsai2025/finetuned-paligemma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/htsai2025/finetuned-paligemma' target=\"_blank\">https://wandb.ai/htsai2025/finetuned-paligemma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/htsai2025/finetuned-paligemma/runs/c56lh1ha' target=\"_blank\">https://wandb.ai/htsai2025/finetuned-paligemma/runs/c56lh1ha</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_activations = torch.load(\"/home/henrytsai/henry/roboterp/activations_fp16.pt\")\n",
    "token_counts = torch.load(\"/home/henrytsai/henry/roboterp/token_counts.pt\")\n",
    "\n",
    "# === Hyperparameters ===\n",
    "hidden_multiplier = 16\n",
    "learning_rate = 1e-4\n",
    "sparsity_weight = 5e-3\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# === Initialize Weights & Biases ===\n",
    "wandb.init(project=\"finetuned-paligemma\", name=\"sunday\", config={\n",
    "    \"hidden_multiplier\": hidden_multiplier,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"sparsity_weight\": sparsity_weight,\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "})\n",
    "\n",
    "# === Prepare SAE ===\n",
    "d_in = all_activations.shape[-1]\n",
    "d_hidden = hidden_multiplier * d_in\n",
    "\n",
    "sae = SparseAutoencoder(d_in=d_in, d_hidden=d_hidden).to(device) \n",
    "optimizer = torch.optim.AdamW(sae.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# === Dataset & Loader ===\n",
    "train_dataset = torch.utils.data.TensorDataset(all_activations)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Half and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch, \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m      6\u001b[39m     batch = batch.to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     recon, z = \u001b[43msae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     loss = loss_fn(recon, batch) + sparsity_weight * torch.mean(torch.abs(z))\n\u001b[32m     10\u001b[39m     epoch_loss += loss.item() * batch.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/henry/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/henry/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mSparseAutoencoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     z = \u001b[38;5;28mself\u001b[39m.activation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    112\u001b[39m     x_recon = \u001b[38;5;28mself\u001b[39m.decoder(z)\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x_recon, z\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/henry/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/henry/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/henry/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 must have the same dtype, but got Half and Float"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch, in train_loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        recon, z = sae(batch)\n",
    "\n",
    "        loss = loss_fn(recon, batch) + sparsity_weight * torch.mean(torch.abs(z))\n",
    "        epoch_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_dataset)\n",
    "    epoch_losses.append(avg_loss)\n",
    "\n",
    "    # Log to wandb every epoch\n",
    "    wandb.log({\"loss\": avg_loss, \"epoch\": epoch + 1})\n",
    "    print(f\"Epoch {epoch}: Loss {avg_loss:.6f}\")\n",
    "\n",
    "print(\"Finished training Sparse Autoencoder!\")\n",
    "\n",
    "# === Save model with timestamp ===\n",
    "save_dir = \"checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model_path = os.path.join(save_dir, f\"sae_{timestamp}.pth\")\n",
    "\n",
    "torch.save({\n",
    "    \"state_dict\": sae.state_dict(),\n",
    "    \"d_in\": d_in,\n",
    "    \"d_hidden\": d_hidden,\n",
    "}, model_path)\n",
    "wandb.save(model_path)\n",
    "\n",
    "print(f\"Saved SAE to {model_path}\")\n",
    "\n",
    "# === Plot loss curve (optional) ===\n",
    "plt.plot(range(1, n_epochs + 1), epoch_losses, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"SAE Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "wandb.log({\"loss_curve\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
