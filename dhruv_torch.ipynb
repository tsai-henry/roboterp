{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned PaliGemma SAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define helper functions and constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForVision2Seq,\n",
    "    AutoProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    "    PretrainedConfig\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import gc\n",
    "import wandb\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "CUDA_DEVICE_INDEX = 3\n",
    "FINETUNE_CHECKPOINT_PATH = \"/home/henrytsai/dhruv/roboterp/finetuned_paligemma.pt\"\n",
    "\n",
    "MODEL_ID = \"google/paligemma2-3b-pt-896\"\n",
    "LAYER_IDX = -1\n",
    "NUM_FRAMES = 6               \n",
    "FRAMES_PER_ROW = 3\n",
    "\n",
    "DROID_DATASET_NAME = \"droid_100\"\n",
    "DROID_DATASET_SPLIT = \"train\"\n",
    "DROID_DATASET_GCS_DIR = \"gs://gresearch/robotics\"\n",
    "NUM_EPISODES = 100\n",
    "NUM_FRAMES = 6\n",
    "TARGET_IMAGE_SIZE = (896, 896)  # Width × Height\n",
    "DIM_IN = 2048\n",
    "DIM_HIDDEN = 4096\n",
    "\n",
    "# === Dataset loading and preprocessing ===\n",
    "def load_droid_subset(num_episodes=100):\n",
    "    ds = tfds.load(DROID_DATASET_NAME, split=DROID_DATASET_SPLIT, data_dir=DROID_DATASET_GCS_DIR)\n",
    "\n",
    "    frames, prompts = [], []\n",
    "    for episode in ds.take(num_episodes):\n",
    "        steps = list(episode[\"steps\"])\n",
    "\n",
    "        # Extract and process wrist camera images\n",
    "        all_imgs = [Image.fromarray(step[\"observation\"][\"wrist_image_left\"].numpy())\n",
    "                    for step in steps]\n",
    "        sampled = subsample_frames(all_imgs, NUM_FRAMES)\n",
    "\n",
    "        # Build grid and resize\n",
    "        grid_pil = stack_images_grid(sampled).resize(TARGET_IMAGE_SIZE, Image.BILINEAR)\n",
    "\n",
    "        frames.append(grid_pil)\n",
    "        prompts.append(steps[0][\"language_instruction\"].numpy().decode(\"utf-8\"))\n",
    "\n",
    "    return frames, prompts\n",
    "\n",
    "vision_acts = {}\n",
    "\n",
    "def vision_hook(module, input, output):\n",
    "    vision_acts[\"activation\"] = output\n",
    "\n",
    "def preprocess_image(im: Image.Image) -> Image.Image:\n",
    "    return im.resize(TARGET_IMAGE_SIZE, Image.BILINEAR)   # <‑‑ used ONLY for display\n",
    "\n",
    "def subsample_frames(images, num_samples):\n",
    "    if len(images) <= num_samples:\n",
    "        return images\n",
    "    idx = np.linspace(0, len(images)-1, num=num_samples, dtype=int)\n",
    "    return [images[i] for i in idx]\n",
    "\n",
    "def stack_images_horizontally(imgs):\n",
    "    w, h = zip(*(im.size for im in imgs))\n",
    "    canvas = Image.new(\"RGB\", (sum(w), max(h)))\n",
    "    x = 0\n",
    "    for im in imgs:\n",
    "        canvas.paste(im, (x, 0))\n",
    "        x += im.width\n",
    "    return canvas\n",
    "\n",
    "def stack_images_grid(imgs, frames_per_row=FRAMES_PER_ROW):\n",
    "    rows = [imgs[i:i+frames_per_row] for i in range(0, len(imgs), frames_per_row)]\n",
    "    rows = [stack_images_horizontally(r) for r in rows]\n",
    "    as_np = np.vstack([np.asarray(r) for r in rows])\n",
    "    return Image.fromarray(as_np)\n",
    "\n",
    "# === Dataset wrapper ===\n",
    "class PromptImageDataset(Dataset):\n",
    "    def __init__(self, frames, prompts):\n",
    "        self.frames = frames\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.frames[idx], self.prompts[idx]\n",
    "\n",
    "# === Custom collate function ===\n",
    "def collate_fn(batch):\n",
    "    batch_frames, batch_prompts = zip(*batch)  # unzip into two lists\n",
    "    return list(batch_frames), list(batch_prompts)\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, d_in=DIM_IN, d_hidden=DIM_HIDDEN):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(d_in, d_hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.decoder = nn.Linear(d_hidden, d_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.activation(self.encoder(x))\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "dtype = torch.float16\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(CUDA_DEVICE_INDEX)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    dtype = torch.bfloat16\n",
    "\n",
    "device = f\"cuda:{CUDA_DEVICE_INDEX}\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load finetuned PaliGemma model (20sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0905262516094710a8759cc913828b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PaliGemmaForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(4096, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
       "    (linear): Linear(in_features=1152, out_features=2304, bias=True)\n",
       "  )\n",
       "  (language_model): Gemma2ForCausalLM(\n",
       "    (model): Gemma2Model(\n",
       "      (embed_tokens): Embedding(257216, 2304, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-25): 26 x Gemma2DecoderLayer(\n",
       "          (self_attn): Gemma2Attention(\n",
       "            (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          )\n",
       "          (mlp): Gemma2MLP(\n",
       "            (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (rotary_emb): Gemma2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2304, out_features=257216, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(f\"cuda:{CUDA_DEVICE_INDEX}\")\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load(FINETUNE_CHECKPOINT_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DROID Dataset (40sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746388667.058563  673128 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 887 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n",
      "I0000 00:00:1746388667.062103  673128 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31496 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:05.0, compute capability: 8.0\n",
      "I0000 00:00:1746388667.064464  673128 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 37640 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:06.0, compute capability: 8.0\n",
      "I0000 00:00:1746388667.066806  673128 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 25012 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:07.0, compute capability: 8.0\n",
      "I0000 00:00:1746388667.069387  673128 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 37216 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:80:00.0, compute capability: 8.0\n",
      "I0000 00:00:1746388667.073212  673128 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 37218 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:80:01.0, compute capability: 8.0\n",
      "I0000 00:00:1746388667.075443  673128 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 20305 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:80:02.0, compute capability: 8.0\n",
      "I0000 00:00:1746388667.077659  673128 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 31955 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:80:03.0, compute capability: 8.0\n",
      "2025-05-04 19:57:47.394536: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-05-04 19:57:47.795781: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-05-04 19:57:48.130032: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-05-04 19:57:48.749874: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-05-04 19:57:50.789870: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-05-04 19:57:52.579851: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-05-04 19:57:56.705991: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-05-04 19:58:05.347374: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "frames, prompts = load_droid_subset(100)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, use_fast=True)\n",
    "hook = model.vision_tower.vision_model.encoder.layers[LAYER_IDX].register_forward_hook(vision_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b633ef07a24882914e2263b3b90f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations: torch.Size([100, 4096, 1152])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1957"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Build dataloader ===\n",
    "dataset = PromptImageDataset(frames, prompts)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "all_activations = []\n",
    "token_counts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_frames, batch_prompts in tqdm(dataloader):\n",
    "        inputs = processor(\n",
    "            text=[f\"<image> {p}\" for p in batch_prompts],\n",
    "            images=batch_frames,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(device, torch.float16)\n",
    "\n",
    "        _ = model(**inputs)\n",
    "\n",
    "        raw_output = vision_acts[\"activation\"]\n",
    "        batch_acts = raw_output[0] if isinstance(raw_output, tuple) else raw_output\n",
    "        batch_acts = batch_acts.detach().cpu()\n",
    "\n",
    "        all_activations.append(batch_acts)\n",
    "        token_counts.append(len(batch_prompts))\n",
    "        \n",
    "# === Combine all activations ===\n",
    "activations = torch.cat(all_activations, dim=0)\n",
    "print(\"Collected activations:\", activations.shape)\n",
    "\n",
    "torch.save(activations, \"activations.pt\")\n",
    "activations_fp16 = activations.half()\n",
    "torch.save(activations_fp16, \"activations_fp16.pt\")\n",
    "torch.save(token_counts, \"token_counts.pt\")\n",
    "\n",
    "del activations\n",
    "vision_acts.clear()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunday</strong> at: <a href='https://wandb.ai/htsai2025/finetuned-paligemma/runs/fi5bevq6' target=\"_blank\">https://wandb.ai/htsai2025/finetuned-paligemma/runs/fi5bevq6</a><br> View project at: <a href='https://wandb.ai/htsai2025/finetuned-paligemma' target=\"_blank\">https://wandb.ai/htsai2025/finetuned-paligemma</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250504_200450-fi5bevq6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/henrytsai/henry/roboterp/wandb/run-20250504_200527-ao5nodex</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/htsai2025/finetuned-paligemma/runs/ao5nodex' target=\"_blank\">sunday</a></strong> to <a href='https://wandb.ai/htsai2025/finetuned-paligemma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/htsai2025/finetuned-paligemma' target=\"_blank\">https://wandb.ai/htsai2025/finetuned-paligemma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/htsai2025/finetuned-paligemma/runs/ao5nodex' target=\"_blank\">https://wandb.ai/htsai2025/finetuned-paligemma/runs/ao5nodex</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_activations = torch.load(\"/home/henrytsai/henry/roboterp/activations.pt\")\n",
    "token_counts = torch.load(\"/home/henrytsai/henry/roboterp/token_counts.pt\")\n",
    "\n",
    "# === Hyperparameters ===\n",
    "hidden_multiplier = 16\n",
    "learning_rate = 1e-4\n",
    "sparsity_weight = 5e-3\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# === Initialize Weights & Biases ===\n",
    "wandb.init(project=\"finetuned-paligemma\", name=\"sunday\", config={\n",
    "    \"hidden_multiplier\": hidden_multiplier,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"sparsity_weight\": sparsity_weight,\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "})\n",
    "\n",
    "# === Prepare SAE ===\n",
    "d_in = all_activations.shape[-1]\n",
    "d_hidden = hidden_multiplier * d_in\n",
    "\n",
    "sae = SparseAutoencoder(d_in=d_in, d_hidden=d_hidden).to(device) \n",
    "optimizer = torch.optim.AdamW(sae.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# === Dataset & Loader ===\n",
    "train_dataset = torch.utils.data.TensorDataset(all_activations)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch, in train_loader:\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        recon, z = sae(batch)\n",
    "\n",
    "        loss = loss_fn(recon, batch) + sparsity_weight * torch.mean(torch.abs(z))\n",
    "        epoch_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_dataset)\n",
    "    epoch_losses.append(avg_loss)\n",
    "\n",
    "    # Log to wandb every epoch\n",
    "    wandb.log({\"loss\": avg_loss, \"epoch\": epoch + 1})\n",
    "    print(f\"Epoch {epoch}: Loss {avg_loss:.6f}\")\n",
    "\n",
    "print(\"Finished training Sparse Autoencoder!\")\n",
    "\n",
    "# === Save model with timestamp ===\n",
    "save_dir = \"checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model_path = os.path.join(save_dir, f\"sae_{timestamp}.pth\")\n",
    "\n",
    "torch.save({\n",
    "    \"state_dict\": sae.state_dict(),\n",
    "    \"d_in\": d_in,\n",
    "    \"d_hidden\": d_hidden,\n",
    "}, model_path)\n",
    "wandb.save(model_path)\n",
    "\n",
    "print(f\"Saved SAE to {model_path}\")\n",
    "\n",
    "# === Plot loss curve (optional) ===\n",
    "plt.plot(range(1, n_epochs + 1), epoch_losses, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"SAE Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "wandb.log({\"loss_curve\": wandb.Image(plt)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
