{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099dcbdf",
   "metadata": {},
   "source": [
    "# Training SAEs with SAELens on Gemma 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c69e1",
   "metadata": {},
   "source": [
    "### Import libraries and detect hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b79e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933afd2",
   "metadata": {},
   "source": [
    "### Load Gemma model checkpoint from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805c60eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0ebea5e8474a46925c6b1f444340d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt = \"google/gemma-3-4b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b62e11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vision_tower\n",
      "vision_tower.vision_model\n",
      "vision_tower.vision_model.embeddings\n",
      "vision_tower.vision_model.embeddings.patch_embedding\n",
      "vision_tower.vision_model.embeddings.position_embedding\n",
      "vision_tower.vision_model.encoder\n",
      "vision_tower.vision_model.encoder.layers\n",
      "vision_tower.vision_model.encoder.layers.0\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.0.mlp\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.1\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.1.mlp\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.2\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.2.mlp\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.3\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.3.mlp\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.4\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.4.mlp\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.5\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.5.mlp\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.6\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.6.mlp\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.7\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.7.mlp\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.8\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.8.mlp\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.9\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.9.mlp\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.10\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.10.mlp\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.11\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.11.mlp\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.12\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.12.mlp\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.13\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.13.mlp\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.14\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.14.mlp\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.15\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.15.mlp\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.16\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.16.mlp\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.17\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.17.mlp\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.18\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.18.mlp\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.19\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.19.mlp\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.20\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.20.mlp\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.21\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.21.mlp\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.22\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.22.mlp\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.23\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.23.mlp\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.24\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.24.mlp\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.25\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.25.mlp\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc2\n",
      "vision_tower.vision_model.encoder.layers.26\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm1\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.k_proj\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.v_proj\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.q_proj\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.out_proj\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm2\n",
      "vision_tower.vision_model.encoder.layers.26.mlp\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.activation_fn\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc1\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc2\n",
      "vision_tower.vision_model.post_layernorm\n",
      "multi_modal_projector\n",
      "multi_modal_projector.mm_soft_emb_norm\n",
      "multi_modal_projector.avg_pool\n",
      "language_model\n",
      "language_model.model\n",
      "language_model.model.embed_tokens\n",
      "language_model.model.layers\n",
      "language_model.model.layers.0\n",
      "language_model.model.layers.0.self_attn\n",
      "language_model.model.layers.0.self_attn.q_proj\n",
      "language_model.model.layers.0.self_attn.k_proj\n",
      "language_model.model.layers.0.self_attn.v_proj\n",
      "language_model.model.layers.0.self_attn.o_proj\n",
      "language_model.model.layers.0.self_attn.q_norm\n",
      "language_model.model.layers.0.self_attn.k_norm\n",
      "language_model.model.layers.0.mlp\n",
      "language_model.model.layers.0.mlp.gate_proj\n",
      "language_model.model.layers.0.mlp.up_proj\n",
      "language_model.model.layers.0.mlp.down_proj\n",
      "language_model.model.layers.0.mlp.act_fn\n",
      "language_model.model.layers.0.input_layernorm\n",
      "language_model.model.layers.0.post_attention_layernorm\n",
      "language_model.model.layers.0.pre_feedforward_layernorm\n",
      "language_model.model.layers.0.post_feedforward_layernorm\n",
      "language_model.model.layers.1\n",
      "language_model.model.layers.1.self_attn\n",
      "language_model.model.layers.1.self_attn.q_proj\n",
      "language_model.model.layers.1.self_attn.k_proj\n",
      "language_model.model.layers.1.self_attn.v_proj\n",
      "language_model.model.layers.1.self_attn.o_proj\n",
      "language_model.model.layers.1.self_attn.q_norm\n",
      "language_model.model.layers.1.self_attn.k_norm\n",
      "language_model.model.layers.1.mlp\n",
      "language_model.model.layers.1.mlp.gate_proj\n",
      "language_model.model.layers.1.mlp.up_proj\n",
      "language_model.model.layers.1.mlp.down_proj\n",
      "language_model.model.layers.1.mlp.act_fn\n",
      "language_model.model.layers.1.input_layernorm\n",
      "language_model.model.layers.1.post_attention_layernorm\n",
      "language_model.model.layers.1.pre_feedforward_layernorm\n",
      "language_model.model.layers.1.post_feedforward_layernorm\n",
      "language_model.model.layers.2\n",
      "language_model.model.layers.2.self_attn\n",
      "language_model.model.layers.2.self_attn.q_proj\n",
      "language_model.model.layers.2.self_attn.k_proj\n",
      "language_model.model.layers.2.self_attn.v_proj\n",
      "language_model.model.layers.2.self_attn.o_proj\n",
      "language_model.model.layers.2.self_attn.q_norm\n",
      "language_model.model.layers.2.self_attn.k_norm\n",
      "language_model.model.layers.2.mlp\n",
      "language_model.model.layers.2.mlp.gate_proj\n",
      "language_model.model.layers.2.mlp.up_proj\n",
      "language_model.model.layers.2.mlp.down_proj\n",
      "language_model.model.layers.2.mlp.act_fn\n",
      "language_model.model.layers.2.input_layernorm\n",
      "language_model.model.layers.2.post_attention_layernorm\n",
      "language_model.model.layers.2.pre_feedforward_layernorm\n",
      "language_model.model.layers.2.post_feedforward_layernorm\n",
      "language_model.model.layers.3\n",
      "language_model.model.layers.3.self_attn\n",
      "language_model.model.layers.3.self_attn.q_proj\n",
      "language_model.model.layers.3.self_attn.k_proj\n",
      "language_model.model.layers.3.self_attn.v_proj\n",
      "language_model.model.layers.3.self_attn.o_proj\n",
      "language_model.model.layers.3.self_attn.q_norm\n",
      "language_model.model.layers.3.self_attn.k_norm\n",
      "language_model.model.layers.3.mlp\n",
      "language_model.model.layers.3.mlp.gate_proj\n",
      "language_model.model.layers.3.mlp.up_proj\n",
      "language_model.model.layers.3.mlp.down_proj\n",
      "language_model.model.layers.3.mlp.act_fn\n",
      "language_model.model.layers.3.input_layernorm\n",
      "language_model.model.layers.3.post_attention_layernorm\n",
      "language_model.model.layers.3.pre_feedforward_layernorm\n",
      "language_model.model.layers.3.post_feedforward_layernorm\n",
      "language_model.model.layers.4\n",
      "language_model.model.layers.4.self_attn\n",
      "language_model.model.layers.4.self_attn.q_proj\n",
      "language_model.model.layers.4.self_attn.k_proj\n",
      "language_model.model.layers.4.self_attn.v_proj\n",
      "language_model.model.layers.4.self_attn.o_proj\n",
      "language_model.model.layers.4.self_attn.q_norm\n",
      "language_model.model.layers.4.self_attn.k_norm\n",
      "language_model.model.layers.4.mlp\n",
      "language_model.model.layers.4.mlp.gate_proj\n",
      "language_model.model.layers.4.mlp.up_proj\n",
      "language_model.model.layers.4.mlp.down_proj\n",
      "language_model.model.layers.4.mlp.act_fn\n",
      "language_model.model.layers.4.input_layernorm\n",
      "language_model.model.layers.4.post_attention_layernorm\n",
      "language_model.model.layers.4.pre_feedforward_layernorm\n",
      "language_model.model.layers.4.post_feedforward_layernorm\n",
      "language_model.model.layers.5\n",
      "language_model.model.layers.5.self_attn\n",
      "language_model.model.layers.5.self_attn.q_proj\n",
      "language_model.model.layers.5.self_attn.k_proj\n",
      "language_model.model.layers.5.self_attn.v_proj\n",
      "language_model.model.layers.5.self_attn.o_proj\n",
      "language_model.model.layers.5.self_attn.q_norm\n",
      "language_model.model.layers.5.self_attn.k_norm\n",
      "language_model.model.layers.5.mlp\n",
      "language_model.model.layers.5.mlp.gate_proj\n",
      "language_model.model.layers.5.mlp.up_proj\n",
      "language_model.model.layers.5.mlp.down_proj\n",
      "language_model.model.layers.5.mlp.act_fn\n",
      "language_model.model.layers.5.input_layernorm\n",
      "language_model.model.layers.5.post_attention_layernorm\n",
      "language_model.model.layers.5.pre_feedforward_layernorm\n",
      "language_model.model.layers.5.post_feedforward_layernorm\n",
      "language_model.model.layers.6\n",
      "language_model.model.layers.6.self_attn\n",
      "language_model.model.layers.6.self_attn.q_proj\n",
      "language_model.model.layers.6.self_attn.k_proj\n",
      "language_model.model.layers.6.self_attn.v_proj\n",
      "language_model.model.layers.6.self_attn.o_proj\n",
      "language_model.model.layers.6.self_attn.q_norm\n",
      "language_model.model.layers.6.self_attn.k_norm\n",
      "language_model.model.layers.6.mlp\n",
      "language_model.model.layers.6.mlp.gate_proj\n",
      "language_model.model.layers.6.mlp.up_proj\n",
      "language_model.model.layers.6.mlp.down_proj\n",
      "language_model.model.layers.6.mlp.act_fn\n",
      "language_model.model.layers.6.input_layernorm\n",
      "language_model.model.layers.6.post_attention_layernorm\n",
      "language_model.model.layers.6.pre_feedforward_layernorm\n",
      "language_model.model.layers.6.post_feedforward_layernorm\n",
      "language_model.model.layers.7\n",
      "language_model.model.layers.7.self_attn\n",
      "language_model.model.layers.7.self_attn.q_proj\n",
      "language_model.model.layers.7.self_attn.k_proj\n",
      "language_model.model.layers.7.self_attn.v_proj\n",
      "language_model.model.layers.7.self_attn.o_proj\n",
      "language_model.model.layers.7.self_attn.q_norm\n",
      "language_model.model.layers.7.self_attn.k_norm\n",
      "language_model.model.layers.7.mlp\n",
      "language_model.model.layers.7.mlp.gate_proj\n",
      "language_model.model.layers.7.mlp.up_proj\n",
      "language_model.model.layers.7.mlp.down_proj\n",
      "language_model.model.layers.7.mlp.act_fn\n",
      "language_model.model.layers.7.input_layernorm\n",
      "language_model.model.layers.7.post_attention_layernorm\n",
      "language_model.model.layers.7.pre_feedforward_layernorm\n",
      "language_model.model.layers.7.post_feedforward_layernorm\n",
      "language_model.model.layers.8\n",
      "language_model.model.layers.8.self_attn\n",
      "language_model.model.layers.8.self_attn.q_proj\n",
      "language_model.model.layers.8.self_attn.k_proj\n",
      "language_model.model.layers.8.self_attn.v_proj\n",
      "language_model.model.layers.8.self_attn.o_proj\n",
      "language_model.model.layers.8.self_attn.q_norm\n",
      "language_model.model.layers.8.self_attn.k_norm\n",
      "language_model.model.layers.8.mlp\n",
      "language_model.model.layers.8.mlp.gate_proj\n",
      "language_model.model.layers.8.mlp.up_proj\n",
      "language_model.model.layers.8.mlp.down_proj\n",
      "language_model.model.layers.8.mlp.act_fn\n",
      "language_model.model.layers.8.input_layernorm\n",
      "language_model.model.layers.8.post_attention_layernorm\n",
      "language_model.model.layers.8.pre_feedforward_layernorm\n",
      "language_model.model.layers.8.post_feedforward_layernorm\n",
      "language_model.model.layers.9\n",
      "language_model.model.layers.9.self_attn\n",
      "language_model.model.layers.9.self_attn.q_proj\n",
      "language_model.model.layers.9.self_attn.k_proj\n",
      "language_model.model.layers.9.self_attn.v_proj\n",
      "language_model.model.layers.9.self_attn.o_proj\n",
      "language_model.model.layers.9.self_attn.q_norm\n",
      "language_model.model.layers.9.self_attn.k_norm\n",
      "language_model.model.layers.9.mlp\n",
      "language_model.model.layers.9.mlp.gate_proj\n",
      "language_model.model.layers.9.mlp.up_proj\n",
      "language_model.model.layers.9.mlp.down_proj\n",
      "language_model.model.layers.9.mlp.act_fn\n",
      "language_model.model.layers.9.input_layernorm\n",
      "language_model.model.layers.9.post_attention_layernorm\n",
      "language_model.model.layers.9.pre_feedforward_layernorm\n",
      "language_model.model.layers.9.post_feedforward_layernorm\n",
      "language_model.model.layers.10\n",
      "language_model.model.layers.10.self_attn\n",
      "language_model.model.layers.10.self_attn.q_proj\n",
      "language_model.model.layers.10.self_attn.k_proj\n",
      "language_model.model.layers.10.self_attn.v_proj\n",
      "language_model.model.layers.10.self_attn.o_proj\n",
      "language_model.model.layers.10.self_attn.q_norm\n",
      "language_model.model.layers.10.self_attn.k_norm\n",
      "language_model.model.layers.10.mlp\n",
      "language_model.model.layers.10.mlp.gate_proj\n",
      "language_model.model.layers.10.mlp.up_proj\n",
      "language_model.model.layers.10.mlp.down_proj\n",
      "language_model.model.layers.10.mlp.act_fn\n",
      "language_model.model.layers.10.input_layernorm\n",
      "language_model.model.layers.10.post_attention_layernorm\n",
      "language_model.model.layers.10.pre_feedforward_layernorm\n",
      "language_model.model.layers.10.post_feedforward_layernorm\n",
      "language_model.model.layers.11\n",
      "language_model.model.layers.11.self_attn\n",
      "language_model.model.layers.11.self_attn.q_proj\n",
      "language_model.model.layers.11.self_attn.k_proj\n",
      "language_model.model.layers.11.self_attn.v_proj\n",
      "language_model.model.layers.11.self_attn.o_proj\n",
      "language_model.model.layers.11.self_attn.q_norm\n",
      "language_model.model.layers.11.self_attn.k_norm\n",
      "language_model.model.layers.11.mlp\n",
      "language_model.model.layers.11.mlp.gate_proj\n",
      "language_model.model.layers.11.mlp.up_proj\n",
      "language_model.model.layers.11.mlp.down_proj\n",
      "language_model.model.layers.11.mlp.act_fn\n",
      "language_model.model.layers.11.input_layernorm\n",
      "language_model.model.layers.11.post_attention_layernorm\n",
      "language_model.model.layers.11.pre_feedforward_layernorm\n",
      "language_model.model.layers.11.post_feedforward_layernorm\n",
      "language_model.model.layers.12\n",
      "language_model.model.layers.12.self_attn\n",
      "language_model.model.layers.12.self_attn.q_proj\n",
      "language_model.model.layers.12.self_attn.k_proj\n",
      "language_model.model.layers.12.self_attn.v_proj\n",
      "language_model.model.layers.12.self_attn.o_proj\n",
      "language_model.model.layers.12.self_attn.q_norm\n",
      "language_model.model.layers.12.self_attn.k_norm\n",
      "language_model.model.layers.12.mlp\n",
      "language_model.model.layers.12.mlp.gate_proj\n",
      "language_model.model.layers.12.mlp.up_proj\n",
      "language_model.model.layers.12.mlp.down_proj\n",
      "language_model.model.layers.12.mlp.act_fn\n",
      "language_model.model.layers.12.input_layernorm\n",
      "language_model.model.layers.12.post_attention_layernorm\n",
      "language_model.model.layers.12.pre_feedforward_layernorm\n",
      "language_model.model.layers.12.post_feedforward_layernorm\n",
      "language_model.model.layers.13\n",
      "language_model.model.layers.13.self_attn\n",
      "language_model.model.layers.13.self_attn.q_proj\n",
      "language_model.model.layers.13.self_attn.k_proj\n",
      "language_model.model.layers.13.self_attn.v_proj\n",
      "language_model.model.layers.13.self_attn.o_proj\n",
      "language_model.model.layers.13.self_attn.q_norm\n",
      "language_model.model.layers.13.self_attn.k_norm\n",
      "language_model.model.layers.13.mlp\n",
      "language_model.model.layers.13.mlp.gate_proj\n",
      "language_model.model.layers.13.mlp.up_proj\n",
      "language_model.model.layers.13.mlp.down_proj\n",
      "language_model.model.layers.13.mlp.act_fn\n",
      "language_model.model.layers.13.input_layernorm\n",
      "language_model.model.layers.13.post_attention_layernorm\n",
      "language_model.model.layers.13.pre_feedforward_layernorm\n",
      "language_model.model.layers.13.post_feedforward_layernorm\n",
      "language_model.model.layers.14\n",
      "language_model.model.layers.14.self_attn\n",
      "language_model.model.layers.14.self_attn.q_proj\n",
      "language_model.model.layers.14.self_attn.k_proj\n",
      "language_model.model.layers.14.self_attn.v_proj\n",
      "language_model.model.layers.14.self_attn.o_proj\n",
      "language_model.model.layers.14.self_attn.q_norm\n",
      "language_model.model.layers.14.self_attn.k_norm\n",
      "language_model.model.layers.14.mlp\n",
      "language_model.model.layers.14.mlp.gate_proj\n",
      "language_model.model.layers.14.mlp.up_proj\n",
      "language_model.model.layers.14.mlp.down_proj\n",
      "language_model.model.layers.14.mlp.act_fn\n",
      "language_model.model.layers.14.input_layernorm\n",
      "language_model.model.layers.14.post_attention_layernorm\n",
      "language_model.model.layers.14.pre_feedforward_layernorm\n",
      "language_model.model.layers.14.post_feedforward_layernorm\n",
      "language_model.model.layers.15\n",
      "language_model.model.layers.15.self_attn\n",
      "language_model.model.layers.15.self_attn.q_proj\n",
      "language_model.model.layers.15.self_attn.k_proj\n",
      "language_model.model.layers.15.self_attn.v_proj\n",
      "language_model.model.layers.15.self_attn.o_proj\n",
      "language_model.model.layers.15.self_attn.q_norm\n",
      "language_model.model.layers.15.self_attn.k_norm\n",
      "language_model.model.layers.15.mlp\n",
      "language_model.model.layers.15.mlp.gate_proj\n",
      "language_model.model.layers.15.mlp.up_proj\n",
      "language_model.model.layers.15.mlp.down_proj\n",
      "language_model.model.layers.15.mlp.act_fn\n",
      "language_model.model.layers.15.input_layernorm\n",
      "language_model.model.layers.15.post_attention_layernorm\n",
      "language_model.model.layers.15.pre_feedforward_layernorm\n",
      "language_model.model.layers.15.post_feedforward_layernorm\n",
      "language_model.model.layers.16\n",
      "language_model.model.layers.16.self_attn\n",
      "language_model.model.layers.16.self_attn.q_proj\n",
      "language_model.model.layers.16.self_attn.k_proj\n",
      "language_model.model.layers.16.self_attn.v_proj\n",
      "language_model.model.layers.16.self_attn.o_proj\n",
      "language_model.model.layers.16.self_attn.q_norm\n",
      "language_model.model.layers.16.self_attn.k_norm\n",
      "language_model.model.layers.16.mlp\n",
      "language_model.model.layers.16.mlp.gate_proj\n",
      "language_model.model.layers.16.mlp.up_proj\n",
      "language_model.model.layers.16.mlp.down_proj\n",
      "language_model.model.layers.16.mlp.act_fn\n",
      "language_model.model.layers.16.input_layernorm\n",
      "language_model.model.layers.16.post_attention_layernorm\n",
      "language_model.model.layers.16.pre_feedforward_layernorm\n",
      "language_model.model.layers.16.post_feedforward_layernorm\n",
      "language_model.model.layers.17\n",
      "language_model.model.layers.17.self_attn\n",
      "language_model.model.layers.17.self_attn.q_proj\n",
      "language_model.model.layers.17.self_attn.k_proj\n",
      "language_model.model.layers.17.self_attn.v_proj\n",
      "language_model.model.layers.17.self_attn.o_proj\n",
      "language_model.model.layers.17.self_attn.q_norm\n",
      "language_model.model.layers.17.self_attn.k_norm\n",
      "language_model.model.layers.17.mlp\n",
      "language_model.model.layers.17.mlp.gate_proj\n",
      "language_model.model.layers.17.mlp.up_proj\n",
      "language_model.model.layers.17.mlp.down_proj\n",
      "language_model.model.layers.17.mlp.act_fn\n",
      "language_model.model.layers.17.input_layernorm\n",
      "language_model.model.layers.17.post_attention_layernorm\n",
      "language_model.model.layers.17.pre_feedforward_layernorm\n",
      "language_model.model.layers.17.post_feedforward_layernorm\n",
      "language_model.model.layers.18\n",
      "language_model.model.layers.18.self_attn\n",
      "language_model.model.layers.18.self_attn.q_proj\n",
      "language_model.model.layers.18.self_attn.k_proj\n",
      "language_model.model.layers.18.self_attn.v_proj\n",
      "language_model.model.layers.18.self_attn.o_proj\n",
      "language_model.model.layers.18.self_attn.q_norm\n",
      "language_model.model.layers.18.self_attn.k_norm\n",
      "language_model.model.layers.18.mlp\n",
      "language_model.model.layers.18.mlp.gate_proj\n",
      "language_model.model.layers.18.mlp.up_proj\n",
      "language_model.model.layers.18.mlp.down_proj\n",
      "language_model.model.layers.18.mlp.act_fn\n",
      "language_model.model.layers.18.input_layernorm\n",
      "language_model.model.layers.18.post_attention_layernorm\n",
      "language_model.model.layers.18.pre_feedforward_layernorm\n",
      "language_model.model.layers.18.post_feedforward_layernorm\n",
      "language_model.model.layers.19\n",
      "language_model.model.layers.19.self_attn\n",
      "language_model.model.layers.19.self_attn.q_proj\n",
      "language_model.model.layers.19.self_attn.k_proj\n",
      "language_model.model.layers.19.self_attn.v_proj\n",
      "language_model.model.layers.19.self_attn.o_proj\n",
      "language_model.model.layers.19.self_attn.q_norm\n",
      "language_model.model.layers.19.self_attn.k_norm\n",
      "language_model.model.layers.19.mlp\n",
      "language_model.model.layers.19.mlp.gate_proj\n",
      "language_model.model.layers.19.mlp.up_proj\n",
      "language_model.model.layers.19.mlp.down_proj\n",
      "language_model.model.layers.19.mlp.act_fn\n",
      "language_model.model.layers.19.input_layernorm\n",
      "language_model.model.layers.19.post_attention_layernorm\n",
      "language_model.model.layers.19.pre_feedforward_layernorm\n",
      "language_model.model.layers.19.post_feedforward_layernorm\n",
      "language_model.model.layers.20\n",
      "language_model.model.layers.20.self_attn\n",
      "language_model.model.layers.20.self_attn.q_proj\n",
      "language_model.model.layers.20.self_attn.k_proj\n",
      "language_model.model.layers.20.self_attn.v_proj\n",
      "language_model.model.layers.20.self_attn.o_proj\n",
      "language_model.model.layers.20.self_attn.q_norm\n",
      "language_model.model.layers.20.self_attn.k_norm\n",
      "language_model.model.layers.20.mlp\n",
      "language_model.model.layers.20.mlp.gate_proj\n",
      "language_model.model.layers.20.mlp.up_proj\n",
      "language_model.model.layers.20.mlp.down_proj\n",
      "language_model.model.layers.20.mlp.act_fn\n",
      "language_model.model.layers.20.input_layernorm\n",
      "language_model.model.layers.20.post_attention_layernorm\n",
      "language_model.model.layers.20.pre_feedforward_layernorm\n",
      "language_model.model.layers.20.post_feedforward_layernorm\n",
      "language_model.model.layers.21\n",
      "language_model.model.layers.21.self_attn\n",
      "language_model.model.layers.21.self_attn.q_proj\n",
      "language_model.model.layers.21.self_attn.k_proj\n",
      "language_model.model.layers.21.self_attn.v_proj\n",
      "language_model.model.layers.21.self_attn.o_proj\n",
      "language_model.model.layers.21.self_attn.q_norm\n",
      "language_model.model.layers.21.self_attn.k_norm\n",
      "language_model.model.layers.21.mlp\n",
      "language_model.model.layers.21.mlp.gate_proj\n",
      "language_model.model.layers.21.mlp.up_proj\n",
      "language_model.model.layers.21.mlp.down_proj\n",
      "language_model.model.layers.21.mlp.act_fn\n",
      "language_model.model.layers.21.input_layernorm\n",
      "language_model.model.layers.21.post_attention_layernorm\n",
      "language_model.model.layers.21.pre_feedforward_layernorm\n",
      "language_model.model.layers.21.post_feedforward_layernorm\n",
      "language_model.model.layers.22\n",
      "language_model.model.layers.22.self_attn\n",
      "language_model.model.layers.22.self_attn.q_proj\n",
      "language_model.model.layers.22.self_attn.k_proj\n",
      "language_model.model.layers.22.self_attn.v_proj\n",
      "language_model.model.layers.22.self_attn.o_proj\n",
      "language_model.model.layers.22.self_attn.q_norm\n",
      "language_model.model.layers.22.self_attn.k_norm\n",
      "language_model.model.layers.22.mlp\n",
      "language_model.model.layers.22.mlp.gate_proj\n",
      "language_model.model.layers.22.mlp.up_proj\n",
      "language_model.model.layers.22.mlp.down_proj\n",
      "language_model.model.layers.22.mlp.act_fn\n",
      "language_model.model.layers.22.input_layernorm\n",
      "language_model.model.layers.22.post_attention_layernorm\n",
      "language_model.model.layers.22.pre_feedforward_layernorm\n",
      "language_model.model.layers.22.post_feedforward_layernorm\n",
      "language_model.model.layers.23\n",
      "language_model.model.layers.23.self_attn\n",
      "language_model.model.layers.23.self_attn.q_proj\n",
      "language_model.model.layers.23.self_attn.k_proj\n",
      "language_model.model.layers.23.self_attn.v_proj\n",
      "language_model.model.layers.23.self_attn.o_proj\n",
      "language_model.model.layers.23.self_attn.q_norm\n",
      "language_model.model.layers.23.self_attn.k_norm\n",
      "language_model.model.layers.23.mlp\n",
      "language_model.model.layers.23.mlp.gate_proj\n",
      "language_model.model.layers.23.mlp.up_proj\n",
      "language_model.model.layers.23.mlp.down_proj\n",
      "language_model.model.layers.23.mlp.act_fn\n",
      "language_model.model.layers.23.input_layernorm\n",
      "language_model.model.layers.23.post_attention_layernorm\n",
      "language_model.model.layers.23.pre_feedforward_layernorm\n",
      "language_model.model.layers.23.post_feedforward_layernorm\n",
      "language_model.model.layers.24\n",
      "language_model.model.layers.24.self_attn\n",
      "language_model.model.layers.24.self_attn.q_proj\n",
      "language_model.model.layers.24.self_attn.k_proj\n",
      "language_model.model.layers.24.self_attn.v_proj\n",
      "language_model.model.layers.24.self_attn.o_proj\n",
      "language_model.model.layers.24.self_attn.q_norm\n",
      "language_model.model.layers.24.self_attn.k_norm\n",
      "language_model.model.layers.24.mlp\n",
      "language_model.model.layers.24.mlp.gate_proj\n",
      "language_model.model.layers.24.mlp.up_proj\n",
      "language_model.model.layers.24.mlp.down_proj\n",
      "language_model.model.layers.24.mlp.act_fn\n",
      "language_model.model.layers.24.input_layernorm\n",
      "language_model.model.layers.24.post_attention_layernorm\n",
      "language_model.model.layers.24.pre_feedforward_layernorm\n",
      "language_model.model.layers.24.post_feedforward_layernorm\n",
      "language_model.model.layers.25\n",
      "language_model.model.layers.25.self_attn\n",
      "language_model.model.layers.25.self_attn.q_proj\n",
      "language_model.model.layers.25.self_attn.k_proj\n",
      "language_model.model.layers.25.self_attn.v_proj\n",
      "language_model.model.layers.25.self_attn.o_proj\n",
      "language_model.model.layers.25.self_attn.q_norm\n",
      "language_model.model.layers.25.self_attn.k_norm\n",
      "language_model.model.layers.25.mlp\n",
      "language_model.model.layers.25.mlp.gate_proj\n",
      "language_model.model.layers.25.mlp.up_proj\n",
      "language_model.model.layers.25.mlp.down_proj\n",
      "language_model.model.layers.25.mlp.act_fn\n",
      "language_model.model.layers.25.input_layernorm\n",
      "language_model.model.layers.25.post_attention_layernorm\n",
      "language_model.model.layers.25.pre_feedforward_layernorm\n",
      "language_model.model.layers.25.post_feedforward_layernorm\n",
      "language_model.model.layers.26\n",
      "language_model.model.layers.26.self_attn\n",
      "language_model.model.layers.26.self_attn.q_proj\n",
      "language_model.model.layers.26.self_attn.k_proj\n",
      "language_model.model.layers.26.self_attn.v_proj\n",
      "language_model.model.layers.26.self_attn.o_proj\n",
      "language_model.model.layers.26.self_attn.q_norm\n",
      "language_model.model.layers.26.self_attn.k_norm\n",
      "language_model.model.layers.26.mlp\n",
      "language_model.model.layers.26.mlp.gate_proj\n",
      "language_model.model.layers.26.mlp.up_proj\n",
      "language_model.model.layers.26.mlp.down_proj\n",
      "language_model.model.layers.26.mlp.act_fn\n",
      "language_model.model.layers.26.input_layernorm\n",
      "language_model.model.layers.26.post_attention_layernorm\n",
      "language_model.model.layers.26.pre_feedforward_layernorm\n",
      "language_model.model.layers.26.post_feedforward_layernorm\n",
      "language_model.model.layers.27\n",
      "language_model.model.layers.27.self_attn\n",
      "language_model.model.layers.27.self_attn.q_proj\n",
      "language_model.model.layers.27.self_attn.k_proj\n",
      "language_model.model.layers.27.self_attn.v_proj\n",
      "language_model.model.layers.27.self_attn.o_proj\n",
      "language_model.model.layers.27.self_attn.q_norm\n",
      "language_model.model.layers.27.self_attn.k_norm\n",
      "language_model.model.layers.27.mlp\n",
      "language_model.model.layers.27.mlp.gate_proj\n",
      "language_model.model.layers.27.mlp.up_proj\n",
      "language_model.model.layers.27.mlp.down_proj\n",
      "language_model.model.layers.27.mlp.act_fn\n",
      "language_model.model.layers.27.input_layernorm\n",
      "language_model.model.layers.27.post_attention_layernorm\n",
      "language_model.model.layers.27.pre_feedforward_layernorm\n",
      "language_model.model.layers.27.post_feedforward_layernorm\n",
      "language_model.model.layers.28\n",
      "language_model.model.layers.28.self_attn\n",
      "language_model.model.layers.28.self_attn.q_proj\n",
      "language_model.model.layers.28.self_attn.k_proj\n",
      "language_model.model.layers.28.self_attn.v_proj\n",
      "language_model.model.layers.28.self_attn.o_proj\n",
      "language_model.model.layers.28.self_attn.q_norm\n",
      "language_model.model.layers.28.self_attn.k_norm\n",
      "language_model.model.layers.28.mlp\n",
      "language_model.model.layers.28.mlp.gate_proj\n",
      "language_model.model.layers.28.mlp.up_proj\n",
      "language_model.model.layers.28.mlp.down_proj\n",
      "language_model.model.layers.28.mlp.act_fn\n",
      "language_model.model.layers.28.input_layernorm\n",
      "language_model.model.layers.28.post_attention_layernorm\n",
      "language_model.model.layers.28.pre_feedforward_layernorm\n",
      "language_model.model.layers.28.post_feedforward_layernorm\n",
      "language_model.model.layers.29\n",
      "language_model.model.layers.29.self_attn\n",
      "language_model.model.layers.29.self_attn.q_proj\n",
      "language_model.model.layers.29.self_attn.k_proj\n",
      "language_model.model.layers.29.self_attn.v_proj\n",
      "language_model.model.layers.29.self_attn.o_proj\n",
      "language_model.model.layers.29.self_attn.q_norm\n",
      "language_model.model.layers.29.self_attn.k_norm\n",
      "language_model.model.layers.29.mlp\n",
      "language_model.model.layers.29.mlp.gate_proj\n",
      "language_model.model.layers.29.mlp.up_proj\n",
      "language_model.model.layers.29.mlp.down_proj\n",
      "language_model.model.layers.29.mlp.act_fn\n",
      "language_model.model.layers.29.input_layernorm\n",
      "language_model.model.layers.29.post_attention_layernorm\n",
      "language_model.model.layers.29.pre_feedforward_layernorm\n",
      "language_model.model.layers.29.post_feedforward_layernorm\n",
      "language_model.model.layers.30\n",
      "language_model.model.layers.30.self_attn\n",
      "language_model.model.layers.30.self_attn.q_proj\n",
      "language_model.model.layers.30.self_attn.k_proj\n",
      "language_model.model.layers.30.self_attn.v_proj\n",
      "language_model.model.layers.30.self_attn.o_proj\n",
      "language_model.model.layers.30.self_attn.q_norm\n",
      "language_model.model.layers.30.self_attn.k_norm\n",
      "language_model.model.layers.30.mlp\n",
      "language_model.model.layers.30.mlp.gate_proj\n",
      "language_model.model.layers.30.mlp.up_proj\n",
      "language_model.model.layers.30.mlp.down_proj\n",
      "language_model.model.layers.30.mlp.act_fn\n",
      "language_model.model.layers.30.input_layernorm\n",
      "language_model.model.layers.30.post_attention_layernorm\n",
      "language_model.model.layers.30.pre_feedforward_layernorm\n",
      "language_model.model.layers.30.post_feedforward_layernorm\n",
      "language_model.model.layers.31\n",
      "language_model.model.layers.31.self_attn\n",
      "language_model.model.layers.31.self_attn.q_proj\n",
      "language_model.model.layers.31.self_attn.k_proj\n",
      "language_model.model.layers.31.self_attn.v_proj\n",
      "language_model.model.layers.31.self_attn.o_proj\n",
      "language_model.model.layers.31.self_attn.q_norm\n",
      "language_model.model.layers.31.self_attn.k_norm\n",
      "language_model.model.layers.31.mlp\n",
      "language_model.model.layers.31.mlp.gate_proj\n",
      "language_model.model.layers.31.mlp.up_proj\n",
      "language_model.model.layers.31.mlp.down_proj\n",
      "language_model.model.layers.31.mlp.act_fn\n",
      "language_model.model.layers.31.input_layernorm\n",
      "language_model.model.layers.31.post_attention_layernorm\n",
      "language_model.model.layers.31.pre_feedforward_layernorm\n",
      "language_model.model.layers.31.post_feedforward_layernorm\n",
      "language_model.model.layers.32\n",
      "language_model.model.layers.32.self_attn\n",
      "language_model.model.layers.32.self_attn.q_proj\n",
      "language_model.model.layers.32.self_attn.k_proj\n",
      "language_model.model.layers.32.self_attn.v_proj\n",
      "language_model.model.layers.32.self_attn.o_proj\n",
      "language_model.model.layers.32.self_attn.q_norm\n",
      "language_model.model.layers.32.self_attn.k_norm\n",
      "language_model.model.layers.32.mlp\n",
      "language_model.model.layers.32.mlp.gate_proj\n",
      "language_model.model.layers.32.mlp.up_proj\n",
      "language_model.model.layers.32.mlp.down_proj\n",
      "language_model.model.layers.32.mlp.act_fn\n",
      "language_model.model.layers.32.input_layernorm\n",
      "language_model.model.layers.32.post_attention_layernorm\n",
      "language_model.model.layers.32.pre_feedforward_layernorm\n",
      "language_model.model.layers.32.post_feedforward_layernorm\n",
      "language_model.model.layers.33\n",
      "language_model.model.layers.33.self_attn\n",
      "language_model.model.layers.33.self_attn.q_proj\n",
      "language_model.model.layers.33.self_attn.k_proj\n",
      "language_model.model.layers.33.self_attn.v_proj\n",
      "language_model.model.layers.33.self_attn.o_proj\n",
      "language_model.model.layers.33.self_attn.q_norm\n",
      "language_model.model.layers.33.self_attn.k_norm\n",
      "language_model.model.layers.33.mlp\n",
      "language_model.model.layers.33.mlp.gate_proj\n",
      "language_model.model.layers.33.mlp.up_proj\n",
      "language_model.model.layers.33.mlp.down_proj\n",
      "language_model.model.layers.33.mlp.act_fn\n",
      "language_model.model.layers.33.input_layernorm\n",
      "language_model.model.layers.33.post_attention_layernorm\n",
      "language_model.model.layers.33.pre_feedforward_layernorm\n",
      "language_model.model.layers.33.post_feedforward_layernorm\n",
      "language_model.model.norm\n",
      "language_model.model.rotary_emb\n",
      "language_model.model.rotary_emb_local\n",
      "language_model.lm_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "ckpt = \"google/gemma-3-1b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(ckpt)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa564335",
   "metadata": {},
   "source": [
    "### Train the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13d811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:19<00:00,  4.88s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]\n",
      "Refilling buffer:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layerwise activations cache size 1\n",
      "Layerwise activations cache:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0: tensor([[[ 0.0188,  0.0005, -0.0202,  ..., -0.0012, -0.0192,  0.0191],\n",
      "         [ 0.0258,  0.0164, -0.0135,  ..., -0.0204, -0.0486, -0.0008],\n",
      "         [ 0.0117,  0.0189, -0.0265,  ..., -0.0045, -0.0053,  0.0067],\n",
      "         ...,\n",
      "         [ 0.0049,  0.0018, -0.0070,  ..., -0.0192, -0.0066,  0.0044],\n",
      "         [-0.0144, -0.0222, -0.0076,  ..., -0.0401, -0.0093, -0.0116],\n",
      "         [-0.0086, -0.0153, -0.0056,  ..., -0.0029,  0.0086,  0.0098]],\n",
      "\n",
      "        [[-0.0018,  0.0136,  0.0078,  ..., -0.0133, -0.0328,  0.0077],\n",
      "         [ 0.0058,  0.0288,  0.0064,  ..., -0.0064, -0.0071,  0.0117],\n",
      "         [ 0.0225,  0.0172,  0.0118,  ...,  0.0279, -0.0157, -0.0129],\n",
      "         ...,\n",
      "         [-0.0065,  0.0066,  0.0042,  ..., -0.0024, -0.0104,  0.0143],\n",
      "         [ 0.0024, -0.0042,  0.0105,  ..., -0.0060,  0.0004,  0.0030],\n",
      "         [ 0.0028,  0.0106,  0.0083,  ...,  0.0153, -0.0151,  0.0037]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1152) must match the existing size (4096) at non-singleton dimension 2.  Target sizes: [2, 2048, 1152].  Tensor sizes: [2, 2048, 4096]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 73\u001b[0m\n\u001b[1;32m      7\u001b[0m l1_warm_up_steps \u001b[38;5;241m=\u001b[39m total_training_steps \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      9\u001b[0m cfg \u001b[38;5;241m=\u001b[39m LanguageModelSAERunnerConfig(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# SAE architecture and model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     architecture\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopk\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# topk used since matryoshka batch topk converts to jumprelu\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     dead_feature_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m\n\u001b[1;32m     71\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m sparse_autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mSAETrainingRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/henry/lib/python3.10/site-packages/sae_lens/sae_training_runner.py:85\u001b[0m, in \u001b[0;36mSAETrainingRunner.__init__\u001b[0;34m(self, cfg, override_dataset, override_model, override_sae)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msae \u001b[38;5;241m=\u001b[39m TrainingSAE(\n\u001b[1;32m     81\u001b[0m             TrainingSAEConfig\u001b[38;5;241m.\u001b[39mfrom_dict(\n\u001b[1;32m     82\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mget_training_sae_cfg_dict(),\n\u001b[1;32m     83\u001b[0m             )\n\u001b[1;32m     84\u001b[0m         )\n\u001b[0;32m---> 85\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_sae_group_b_decs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msae \u001b[38;5;241m=\u001b[39m override_sae\n",
      "File \u001b[0;32m~/anaconda3/envs/henry/lib/python3.10/site-packages/sae_lens/sae_training_runner.py:172\u001b[0m, in \u001b[0;36mSAETrainingRunner._init_sae_group_b_decs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mb_dec_init_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometric_median\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations_store\u001b[38;5;241m.\u001b[39mset_norm_scaling_factor_if_needed()\n\u001b[0;32m--> 172\u001b[0m     layer_acts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_buffer\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# get geometric median of the activations if we're using those.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     median \u001b[38;5;241m=\u001b[39m compute_geometric_median(\n\u001b[1;32m    175\u001b[0m         layer_acts,\n\u001b[1;32m    176\u001b[0m         maxiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m    177\u001b[0m     )\u001b[38;5;241m.\u001b[39mmedian\n",
      "File \u001b[0;32m~/anaconda3/envs/henry/lib/python3.10/site-packages/sae_lens/training/activations_store.py:481\u001b[0m, in \u001b[0;36mActivationsStore.storage_buffer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstorage_buffer\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_buffer \u001b[38;5;241m=\u001b[39m _filter_buffer_acts(\n\u001b[0;32m--> 481\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf_buffer_size\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_special_tokens\n\u001b[1;32m    482\u001b[0m         )\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_buffer\n",
      "File \u001b[0;32m~/anaconda3/envs/henry/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/henry/lib/python3.10/site-packages/sae_lens/training/activations_store.py:695\u001b[0m, in \u001b[0;36mActivationsStore.get_buffer\u001b[0;34m(self, n_batches_in_buffer, raise_on_epoch_end, shuffle)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m refill_batch_idx_start \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m    689\u001b[0m     refill_iterator, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefilling buffer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    690\u001b[0m ):\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# move batch toks to gpu for model\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     refill_batch_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_tokens(\n\u001b[1;32m    693\u001b[0m         raise_at_epoch_end\u001b[38;5;241m=\u001b[39mraise_on_epoch_end\n\u001b[1;32m    694\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(_get_model_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel))\n\u001b[0;32m--> 695\u001b[0m     refill_activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefill_batch_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;66;03m# move acts back to cpu\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     refill_activations\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/henry/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/henry/lib/python3.10/site-packages/sae_lens/training/activations_store.py:573\u001b[0m, in \u001b[0;36mActivationsStore.get_activations\u001b[0;34m(self, batch_tokens)\u001b[0m\n\u001b[1;32m    569\u001b[0m         stacked_activations[:, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m layerwise_activations\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    570\u001b[0m             n_batches, n_context, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    571\u001b[0m         )\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m     \u001b[43mstacked_activations\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m layerwise_activations\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stacked_activations\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1152) must match the existing size (4096) at non-singleton dimension 2.  Target sizes: [2, 2048, 1152].  Tensor sizes: [2, 2048, 4096]"
     ]
    }
   ],
   "source": [
    "total_training_steps = 100_000\n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5\n",
    "l1_warm_up_steps = total_training_steps // 20\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # SAE architecture and model\n",
    "    architecture=\"topk\",  # topk used since matryoshka batch topk converts to jumprelu\n",
    "    activation_fn=\"topk\",\n",
    "    activation_fn_kwargs={\"k\": 40},  # important: this is where 'k' goes\n",
    "    model_name=\"google/gemma-3-1b-pt\", # lol it wasn't working for so long because it didn't have google in front of it \n",
    "    model_class_name=\"AutoModelForCausalLM\",\n",
    "    hook_name=\"model.layers.0\",\n",
    "    hook_layer=0,\n",
    "    d_in=1152,\n",
    "    d_sae=32768,\n",
    "\n",
    "    # Dataset\n",
    "    dataset_path=\"apollo-research/monology-pile-uncopyrighted-tokenizer-EleutherAI-gpt-neox-20b\",\n",
    "    dataset_trust_remote_code=True,\n",
    "    streaming=True,\n",
    "    context_size=2048,\n",
    "    prepend_bos=True,\n",
    "\n",
    "    # Training\n",
    "    lr=3e-4,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",\n",
    "    lr_warm_up_steps=lr_warm_up_steps,\n",
    "    lr_decay_steps=lr_decay_steps,\n",
    "    l1_coefficient=0.001,\n",
    "    l1_warm_up_steps=l1_warm_up_steps,\n",
    "    lp_norm=1.0,\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    training_tokens=total_training_tokens,\n",
    "    n_batches_in_buffer=4,\n",
    "    store_batch_size_prompts=2,\n",
    "\n",
    "    # Init + Heuristics\n",
    "    apply_b_dec_to_input=True,\n",
    "    decoder_orthogonal_init=False,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    normalize_sae_decoder=False,\n",
    "    normalize_activations=\"none\",\n",
    "    exclude_special_tokens=True,\n",
    "\n",
    "    # Logging\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"sae_gemma3_experiment\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "\n",
    "    # Misc\n",
    "    device=\"cuda\",\n",
    "    act_store_device=\"with_model\",\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    verbose=True,\n",
    "\n",
    "    # Stability\n",
    "    use_ghost_grads=False,\n",
    "    feature_sampling_window=1000,\n",
    "    dead_feature_window=1000,\n",
    "    dead_feature_threshold=1e-4\n",
    ")\n",
    "\n",
    "sparse_autoencoder = SAETrainingRunner(cfg).run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722c087",
   "metadata": {},
   "source": [
    "### [Deletes the model] Clean up the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9b3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del sparse_autoencoder  # Uncomment to delete the trained SAE\n",
    "gc.collect() # Restart the kernel if there is still leftover memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22b62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
