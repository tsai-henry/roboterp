{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099dcbdf",
   "metadata": {},
   "source": [
    "# Training SAEs with SAELens on Gemma 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c69e1",
   "metadata": {},
   "source": [
    "### Import libraries and detect hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b79e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933afd2",
   "metadata": {},
   "source": [
    "### Load Gemma model checkpoint from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805c60eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "ckpt = \"google/gemma-3-4b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa564335",
   "metadata": {},
   "source": [
    "### Train the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13d811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective value: 2408587.0000:   8%|▊         | 8/100 [00:00<00:00, 725.52it/s]\n",
      "/home/henrytsai/anaconda3/envs/henry/lib/python3.10/site-packages/sae_lens/training/training_sae.py:636: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhtsai\u001b[0m (\u001b[33mhtsai2025\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/henrytsai/henry/roboterp/wandb/run-20250424_234729-c7rqv10m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/htsai2025/sae_gemma3_experiment/runs/c7rqv10m' target=\"_blank\">32768-L1-0.001-LR-0.0003-Tokens-1.229e+08</a></strong> to <a href='https://wandb.ai/htsai2025/sae_gemma3_experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/htsai2025/sae_gemma3_experiment' target=\"_blank\">https://wandb.ai/htsai2025/sae_gemma3_experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/htsai2025/sae_gemma3_experiment/runs/c7rqv10m' target=\"_blank\">https://wandb.ai/htsai2025/sae_gemma3_experiment/runs/c7rqv10m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200| auxiliary_reconstruction_loss: 0.00000 | mse_loss: 11170.60254:   1%|          | 819200/122880000 [02:36<5:49:45, 5816.30it/s]"
     ]
    }
   ],
   "source": [
    "total_training_steps = 30_000\n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5\n",
    "l1_warm_up_steps = total_training_steps // 20\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # SAE architecture and model\n",
    "    architecture=\"topk\",  # topk used since matryoshka batch topk converts to jumprelu\n",
    "    activation_fn=\"topk\",\n",
    "    activation_fn_kwargs={\"k\": 40},  # important: this is where 'k' goes\n",
    "    model_name=\"google/gemma-3-1b-pt\", # lol it wasn't working for so long because it didn't have google in front of it \n",
    "    model_class_name=\"AutoModelForCausalLM\",\n",
    "    hook_name=\"model.layers.0\",\n",
    "    hook_layer=0,\n",
    "    d_in=1152,\n",
    "    d_sae=32768,\n",
    "\n",
    "    # Dataset\n",
    "    dataset_path=\"apollo-research/monology-pile-uncopyrighted-tokenizer-EleutherAI-gpt-neox-20b\",\n",
    "    dataset_trust_remote_code=True,\n",
    "    streaming=True,\n",
    "    context_size=2048,\n",
    "    prepend_bos=True,\n",
    "\n",
    "    # Training\n",
    "    lr=3e-4,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",\n",
    "    lr_warm_up_steps=lr_warm_up_steps,\n",
    "    lr_decay_steps=lr_decay_steps,\n",
    "    l1_coefficient=0.001,\n",
    "    l1_warm_up_steps=l1_warm_up_steps,\n",
    "    lp_norm=1.0,\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    training_tokens=total_training_tokens,\n",
    "    n_batches_in_buffer=4,\n",
    "    store_batch_size_prompts=2,\n",
    "\n",
    "    # Init + Heuristics\n",
    "    apply_b_dec_to_input=True,\n",
    "    decoder_orthogonal_init=False,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    normalize_sae_decoder=False,\n",
    "    normalize_activations=\"none\",\n",
    "    exclude_special_tokens=True,\n",
    "\n",
    "    # Logging\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"sae_gemma3_experiment\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "\n",
    "    # Misc\n",
    "    device=\"cuda\",\n",
    "    act_store_device=\"with_model\",\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    verbose=True,\n",
    "\n",
    "    # Stability\n",
    "    use_ghost_grads=False,\n",
    "    feature_sampling_window=1000,\n",
    "    dead_feature_window=1000,\n",
    "    dead_feature_threshold=1e-4\n",
    ")\n",
    "\n",
    "sparse_autoencoder = SAETrainingRunner(cfg).run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722c087",
   "metadata": {},
   "source": [
    "### [Deletes the model] Clean up the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b3060",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "del sparse_autoencoder  # or whatever large object you want to remove\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22b62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
