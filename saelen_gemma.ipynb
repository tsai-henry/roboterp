{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099dcbdf",
   "metadata": {},
   "source": [
    "# Training SAEs with SAELens on Gemma 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c69e1",
   "metadata": {},
   "source": [
    "### Import libraries and detect hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9b79e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933afd2",
   "metadata": {},
   "source": [
    "### Load Gemma model checkpoint from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805c60eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = \"google/gemma-3-1b-pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa564335",
   "metadata": {},
   "source": [
    "### Train the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce13d811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 303/303 [00:00<00:00, 6.61kB/s]\n",
      "Objective value: 2408587.0000:   6%|▌         | 6/100 [00:00<00:00, 652.67it/s]\n",
      "/home/henrytsai/anaconda3/envs/henry/lib/python3.10/site-packages/sae_lens/training/training_sae.py:636: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhtsai\u001b[0m (\u001b[33mhtsai2025\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (9.9s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/henrytsai/henry/roboterp/wandb/run-20250424_223317-44jxb06u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/htsai2025/sae_gemma3_experiment/runs/44jxb06u' target=\"_blank\">32768-L1-0.001-LR-0.0003-Tokens-1.024e+06</a></strong> to <a href='https://wandb.ai/htsai2025/sae_gemma3_experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/htsai2025/sae_gemma3_experiment' target=\"_blank\">https://wandb.ai/htsai2025/sae_gemma3_experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/htsai2025/sae_gemma3_experiment/runs/44jxb06u' target=\"_blank\">https://wandb.ai/htsai2025/sae_gemma3_experiment/runs/44jxb06u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000| auxiliary_reconstruction_loss: 0.00000 | mse_loss: 6400.62744: 100%|██████████| 1024000/1024000 [03:22<00:00, 5046.94it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>▁████████████████████████████████</td></tr><tr><td>details/current_learning_rate</td><td>███████████████████████████▇▅▄▃▂▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▆▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▆▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance</td><td>▁▃▄▆▆▇▇▇▇▇▇██████████████████████</td></tr><tr><td>metrics/explained_variance_legacy</td><td>▁▃▅▆▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>metrics/explained_variance_legacy_std</td><td>▃▆███▇▆▅▄▄▃▃▃▃▂▃▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▂▁</td></tr><tr><td>metrics/l0</td><td>▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁</td></tr><tr><td>sparsity/below_1e-6</td><td>▁</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient</td><td>0.001</td></tr><tr><td>details/current_learning_rate</td><td>2e-05</td></tr><tr><td>details/n_training_tokens</td><td>1013760</td></tr><tr><td>losses/auxiliary_reconstruction_loss</td><td>0</td></tr><tr><td>losses/mse_loss</td><td>6181.89844</td></tr><tr><td>losses/overall_loss</td><td>6181.89844</td></tr><tr><td>metrics/explained_variance</td><td>0.9318</td></tr><tr><td>metrics/explained_variance_legacy</td><td>0.93539</td></tr><tr><td>metrics/explained_variance_legacy_std</td><td>0.0778</td></tr><tr><td>metrics/l0</td><td>40</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-8.19449</td></tr><tr><td>sparsity/below_1e-5</td><td>25945</td></tr><tr><td>sparsity/below_1e-6</td><td>23598</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>868.37097</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">32768-L1-0.001-LR-0.0003-Tokens-1.024e+06</strong> at: <a href='https://wandb.ai/htsai2025/sae_gemma3_experiment/runs/44jxb06u' target=\"_blank\">https://wandb.ai/htsai2025/sae_gemma3_experiment/runs/44jxb06u</a><br> View project at: <a href='https://wandb.ai/htsai2025/sae_gemma3_experiment' target=\"_blank\">https://wandb.ai/htsai2025/sae_gemma3_experiment</a><br>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250424_223317-44jxb06u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_training_steps = 1000\n",
    "batch_size = 1024\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5\n",
    "l1_warm_up_steps = total_training_steps // 20\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # SAE architecture and model\n",
    "    architecture=\"topk\",  # topk used since matryoshka batch topk converts to jumprelu\n",
    "    activation_fn=\"topk\",\n",
    "    activation_fn_kwargs={\"k\": 40},  # important: this is where 'k' goes\n",
    "    model_name=\"google/gemma-3-1b-pt\", # lol it wasn't working for so long because it didn't have google in front of it \n",
    "    model_class_name=\"AutoModelForCausalLM\",\n",
    "    hook_name=\"model.layers.0\",\n",
    "    hook_layer=0,\n",
    "    d_in=1152,\n",
    "    d_sae=32768,\n",
    "\n",
    "    # Dataset\n",
    "    dataset_path=\"apollo-research/monology-pile-uncopyrighted-tokenizer-EleutherAI-gpt-neox-20b\",\n",
    "    dataset_trust_remote_code=True,\n",
    "    streaming=True,\n",
    "    context_size=2048,\n",
    "    prepend_bos=True,\n",
    "\n",
    "    # Training\n",
    "    lr=3e-4,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",\n",
    "    lr_warm_up_steps=lr_warm_up_steps,\n",
    "    lr_decay_steps=lr_decay_steps,\n",
    "    l1_coefficient=0.001,\n",
    "    l1_warm_up_steps=l1_warm_up_steps,\n",
    "    lp_norm=1.0,\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    training_tokens=total_training_tokens,\n",
    "    n_batches_in_buffer=4,\n",
    "    store_batch_size_prompts=2,\n",
    "\n",
    "    # Init + Heuristics\n",
    "    apply_b_dec_to_input=True,\n",
    "    decoder_orthogonal_init=False,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    normalize_sae_decoder=False,\n",
    "    normalize_activations=\"none\",\n",
    "    exclude_special_tokens=True,\n",
    "\n",
    "    # Logging\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"sae_gemma3_experiment\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "\n",
    "    # Misc\n",
    "    device=\"cuda\",\n",
    "    act_store_device=\"with_model\",\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    verbose=True,\n",
    "\n",
    "    # Stability\n",
    "    use_ghost_grads=False,\n",
    "    feature_sampling_window=1000,\n",
    "    dead_feature_window=1000,\n",
    "    dead_feature_threshold=1e-4\n",
    ")\n",
    "\n",
    "sparse_autoencoder = SAETrainingRunner(cfg).run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722c087",
   "metadata": {},
   "source": [
    "### Clean up the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c9b3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model  # or whatever large object you want to remove\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793982f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
